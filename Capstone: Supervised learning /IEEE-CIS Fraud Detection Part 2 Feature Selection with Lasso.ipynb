{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IEEE-CIS Fraud Detection Part 2\n",
    "\n",
    "In this series of notebooks, we are working on a supervised, regression machine learning problem. Using Kaggle's competition [IEEE-CIS Fraud Detection](https://www.kaggle.com/c/ieee-fraud-detection) dataset, we want to predict whether a transaction is fraud or not. \n",
    "\n",
    " ### Workflow \n",
    " 1. Understand the problem (we're almost there already)\n",
    " 2. Exploratory Data Analysis\n",
    " 3. Feature engineering to create a dataset for machine learning\n",
    " 4. Create a baseline machine learning model\n",
    " 5. Try more complex machine learning models\n",
    " 6. Optimize the selected model\n",
    " 7. Investigate model predictions in context of problem\n",
    " 8. Draw conclusions and lay out next steps\n",
    " \n",
    "The first notebook covered steps 1-3, and in this notebook, we will cover 4-6."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read in Data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numpy and pandas\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "#Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import missingno as msno\n",
    "\n",
    "# Statistics tools\n",
    "import scipy.stats as stats\n",
    "\n",
    "# Sklearn data clean\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Model selection\n",
    "from sklearn.decomposition import PCA \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# Logistic Regression\n",
    "from sklearn.linear_model import Lasso, LogisticRegression\n",
    "\n",
    "# KNN Classifer \n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Decision Trees\n",
    "from sklearn import tree\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from IPython.display import Image\n",
    "import pydotplus\n",
    "import graphviz\n",
    "\n",
    "# Random Forests \n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# SVM\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Gradient Boost\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# Evaluate\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import log_loss,accuracy_score, f1_score,roc_auc_score, confusion_matrix\n",
    "\n",
    "# Import data\n",
    "from sqlalchemy import create_engine\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Data\n",
    "train_df = pd.read_csv('/Users/tsawaengsri/Desktop/Data Science Courses/Datasets/ieee-fraud-detection/train.csv')\n",
    "test_df = pd.read_csv('/Users/tsawaengsri/Desktop/Data Science Courses/Datasets/ieee-fraud-detection/test.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Feature Size:  (590540, 2161)\n",
      "Testing Feature Size:   (506691, 2160)\n"
     ]
    }
   ],
   "source": [
    "# Display sizes of data\n",
    "print('Training Feature Size: ', train.shape)\n",
    "print('Testing Feature Size:  ', test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TransactionID</th>\n",
       "      <th>TransactionDT</th>\n",
       "      <th>TransactionAmt</th>\n",
       "      <th>card1</th>\n",
       "      <th>card2</th>\n",
       "      <th>card3</th>\n",
       "      <th>card5</th>\n",
       "      <th>addr1</th>\n",
       "      <th>addr2</th>\n",
       "      <th>dist1</th>\n",
       "      <th>...</th>\n",
       "      <th>Transaction_hour</th>\n",
       "      <th>card1_TransactionAmt_mean</th>\n",
       "      <th>card1_TransactionAmt_std</th>\n",
       "      <th>card2_TransactionAmt_mean</th>\n",
       "      <th>card2_TransactionAmt_std</th>\n",
       "      <th>card3_TransactionAmt_mean</th>\n",
       "      <th>card3_TransactionAmt_std</th>\n",
       "      <th>card5_TransactionAmt_mean</th>\n",
       "      <th>card5_TransactionAmt_std</th>\n",
       "      <th>isFraud</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2987000</td>\n",
       "      <td>86400</td>\n",
       "      <td>68.5</td>\n",
       "      <td>13926</td>\n",
       "      <td>NaN</td>\n",
       "      <td>150.0</td>\n",
       "      <td>142.0</td>\n",
       "      <td>315.0</td>\n",
       "      <td>87.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>316.570357</td>\n",
       "      <td>351.513997</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>147.65346</td>\n",
       "      <td>255.330369</td>\n",
       "      <td>185.236343</td>\n",
       "      <td>322.134467</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2987001</td>\n",
       "      <td>86401</td>\n",
       "      <td>29.0</td>\n",
       "      <td>2755</td>\n",
       "      <td>404.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>102.0</td>\n",
       "      <td>325.0</td>\n",
       "      <td>87.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>213.053819</td>\n",
       "      <td>391.543884</td>\n",
       "      <td>227.107106</td>\n",
       "      <td>373.703941</td>\n",
       "      <td>147.65346</td>\n",
       "      <td>255.330369</td>\n",
       "      <td>212.793700</td>\n",
       "      <td>396.390243</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2987002</td>\n",
       "      <td>86469</td>\n",
       "      <td>59.0</td>\n",
       "      <td>4663</td>\n",
       "      <td>490.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>166.0</td>\n",
       "      <td>330.0</td>\n",
       "      <td>87.0</td>\n",
       "      <td>287.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>104.876940</td>\n",
       "      <td>130.380968</td>\n",
       "      <td>136.179809</td>\n",
       "      <td>228.571548</td>\n",
       "      <td>147.65346</td>\n",
       "      <td>255.330369</td>\n",
       "      <td>98.774960</td>\n",
       "      <td>141.059909</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2987003</td>\n",
       "      <td>86499</td>\n",
       "      <td>50.0</td>\n",
       "      <td>18132</td>\n",
       "      <td>567.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>117.0</td>\n",
       "      <td>476.0</td>\n",
       "      <td>87.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>120.958705</td>\n",
       "      <td>196.463487</td>\n",
       "      <td>133.628801</td>\n",
       "      <td>226.771834</td>\n",
       "      <td>147.65346</td>\n",
       "      <td>255.330369</td>\n",
       "      <td>124.389514</td>\n",
       "      <td>191.880900</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2987004</td>\n",
       "      <td>86506</td>\n",
       "      <td>50.0</td>\n",
       "      <td>4497</td>\n",
       "      <td>514.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>102.0</td>\n",
       "      <td>420.0</td>\n",
       "      <td>87.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>99.811667</td>\n",
       "      <td>69.829736</td>\n",
       "      <td>223.770752</td>\n",
       "      <td>457.894839</td>\n",
       "      <td>147.65346</td>\n",
       "      <td>255.330369</td>\n",
       "      <td>212.793700</td>\n",
       "      <td>396.390243</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 2161 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   TransactionID  TransactionDT  TransactionAmt  card1  card2  card3  card5  \\\n",
       "0        2987000          86400            68.5  13926    NaN  150.0  142.0   \n",
       "1        2987001          86401            29.0   2755  404.0  150.0  102.0   \n",
       "2        2987002          86469            59.0   4663  490.0  150.0  166.0   \n",
       "3        2987003          86499            50.0  18132  567.0  150.0  117.0   \n",
       "4        2987004          86506            50.0   4497  514.0  150.0  102.0   \n",
       "\n",
       "   addr1  addr2  dist1  ...  Transaction_hour  card1_TransactionAmt_mean  \\\n",
       "0  315.0   87.0   19.0  ...               0.0                 316.570357   \n",
       "1  325.0   87.0    NaN  ...               0.0                 213.053819   \n",
       "2  330.0   87.0  287.0  ...               0.0                 104.876940   \n",
       "3  476.0   87.0    NaN  ...               0.0                 120.958705   \n",
       "4  420.0   87.0    NaN  ...               0.0                  99.811667   \n",
       "\n",
       "   card1_TransactionAmt_std  card2_TransactionAmt_mean  \\\n",
       "0                351.513997                        NaN   \n",
       "1                391.543884                 227.107106   \n",
       "2                130.380968                 136.179809   \n",
       "3                196.463487                 133.628801   \n",
       "4                 69.829736                 223.770752   \n",
       "\n",
       "   card2_TransactionAmt_std  card3_TransactionAmt_mean  \\\n",
       "0                       NaN                  147.65346   \n",
       "1                373.703941                  147.65346   \n",
       "2                228.571548                  147.65346   \n",
       "3                226.771834                  147.65346   \n",
       "4                457.894839                  147.65346   \n",
       "\n",
       "   card3_TransactionAmt_std  card5_TransactionAmt_mean  \\\n",
       "0                255.330369                 185.236343   \n",
       "1                255.330369                 212.793700   \n",
       "2                255.330369                  98.774960   \n",
       "3                255.330369                 124.389514   \n",
       "4                255.330369                 212.793700   \n",
       "\n",
       "   card5_TransactionAmt_std  isFraud  \n",
       "0                322.134467        0  \n",
       "1                396.390243        0  \n",
       "2                141.059909        0  \n",
       "3                191.880900        0  \n",
       "4                396.390243        0  \n",
       "\n",
       "[5 rows x 2161 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 590540 entries, 0 to 590539\n",
      "Columns: 2161 entries, TransactionID to isFraud\n",
      "dtypes: float64(410), int64(1751)\n",
      "memory usage: 9.5 GB\n"
     ]
    }
   ],
   "source": [
    "train_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TransactionID</th>\n",
       "      <th>TransactionDT</th>\n",
       "      <th>TransactionAmt</th>\n",
       "      <th>card1</th>\n",
       "      <th>card2</th>\n",
       "      <th>card3</th>\n",
       "      <th>card5</th>\n",
       "      <th>addr1</th>\n",
       "      <th>addr2</th>\n",
       "      <th>dist1</th>\n",
       "      <th>...</th>\n",
       "      <th>Transaction_day_of_week</th>\n",
       "      <th>Transaction_hour</th>\n",
       "      <th>card1_TransactionAmt_mean</th>\n",
       "      <th>card1_TransactionAmt_std</th>\n",
       "      <th>card2_TransactionAmt_mean</th>\n",
       "      <th>card2_TransactionAmt_std</th>\n",
       "      <th>card3_TransactionAmt_mean</th>\n",
       "      <th>card3_TransactionAmt_std</th>\n",
       "      <th>card5_TransactionAmt_mean</th>\n",
       "      <th>card5_TransactionAmt_std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3663549</td>\n",
       "      <td>18403224</td>\n",
       "      <td>31.95</td>\n",
       "      <td>10409</td>\n",
       "      <td>111.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>226.0</td>\n",
       "      <td>170.0</td>\n",
       "      <td>87.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>111.438993</td>\n",
       "      <td>127.021956</td>\n",
       "      <td>150.513374</td>\n",
       "      <td>258.890796</td>\n",
       "      <td>147.65346</td>\n",
       "      <td>255.330369</td>\n",
       "      <td>141.865993</td>\n",
       "      <td>242.457293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3663550</td>\n",
       "      <td>18403263</td>\n",
       "      <td>49.00</td>\n",
       "      <td>4272</td>\n",
       "      <td>111.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>226.0</td>\n",
       "      <td>299.0</td>\n",
       "      <td>87.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>154.212902</td>\n",
       "      <td>315.255714</td>\n",
       "      <td>150.513374</td>\n",
       "      <td>258.890796</td>\n",
       "      <td>147.65346</td>\n",
       "      <td>255.330369</td>\n",
       "      <td>141.865993</td>\n",
       "      <td>242.457293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3663551</td>\n",
       "      <td>18403310</td>\n",
       "      <td>171.00</td>\n",
       "      <td>4476</td>\n",
       "      <td>574.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>226.0</td>\n",
       "      <td>472.0</td>\n",
       "      <td>87.0</td>\n",
       "      <td>2635.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>137.671446</td>\n",
       "      <td>134.993751</td>\n",
       "      <td>162.600916</td>\n",
       "      <td>161.299536</td>\n",
       "      <td>147.65346</td>\n",
       "      <td>255.330369</td>\n",
       "      <td>141.865993</td>\n",
       "      <td>242.457293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3663552</td>\n",
       "      <td>18403310</td>\n",
       "      <td>284.95</td>\n",
       "      <td>10989</td>\n",
       "      <td>360.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>166.0</td>\n",
       "      <td>205.0</td>\n",
       "      <td>87.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>89.571312</td>\n",
       "      <td>130.253829</td>\n",
       "      <td>97.760244</td>\n",
       "      <td>144.617870</td>\n",
       "      <td>147.65346</td>\n",
       "      <td>255.330369</td>\n",
       "      <td>98.774960</td>\n",
       "      <td>141.059909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3663553</td>\n",
       "      <td>18403317</td>\n",
       "      <td>67.95</td>\n",
       "      <td>18018</td>\n",
       "      <td>452.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>117.0</td>\n",
       "      <td>264.0</td>\n",
       "      <td>87.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>115.629317</td>\n",
       "      <td>214.010137</td>\n",
       "      <td>117.029127</td>\n",
       "      <td>210.349436</td>\n",
       "      <td>147.65346</td>\n",
       "      <td>255.330369</td>\n",
       "      <td>124.389514</td>\n",
       "      <td>191.880900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 2160 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   TransactionID  TransactionDT  TransactionAmt  card1  card2  card3  card5  \\\n",
       "0        3663549       18403224           31.95  10409  111.0  150.0  226.0   \n",
       "1        3663550       18403263           49.00   4272  111.0  150.0  226.0   \n",
       "2        3663551       18403310          171.00   4476  574.0  150.0  226.0   \n",
       "3        3663552       18403310          284.95  10989  360.0  150.0  166.0   \n",
       "4        3663553       18403317           67.95  18018  452.0  150.0  117.0   \n",
       "\n",
       "   addr1  addr2   dist1  ...  Transaction_day_of_week  Transaction_hour  \\\n",
       "0  170.0   87.0     1.0  ...                      2.0               0.0   \n",
       "1  299.0   87.0     4.0  ...                      2.0               0.0   \n",
       "2  472.0   87.0  2635.0  ...                      2.0               0.0   \n",
       "3  205.0   87.0    17.0  ...                      2.0               0.0   \n",
       "4  264.0   87.0     6.0  ...                      2.0               0.0   \n",
       "\n",
       "   card1_TransactionAmt_mean  card1_TransactionAmt_std  \\\n",
       "0                 111.438993                127.021956   \n",
       "1                 154.212902                315.255714   \n",
       "2                 137.671446                134.993751   \n",
       "3                  89.571312                130.253829   \n",
       "4                 115.629317                214.010137   \n",
       "\n",
       "   card2_TransactionAmt_mean  card2_TransactionAmt_std  \\\n",
       "0                 150.513374                258.890796   \n",
       "1                 150.513374                258.890796   \n",
       "2                 162.600916                161.299536   \n",
       "3                  97.760244                144.617870   \n",
       "4                 117.029127                210.349436   \n",
       "\n",
       "   card3_TransactionAmt_mean  card3_TransactionAmt_std  \\\n",
       "0                  147.65346                255.330369   \n",
       "1                  147.65346                255.330369   \n",
       "2                  147.65346                255.330369   \n",
       "3                  147.65346                255.330369   \n",
       "4                  147.65346                255.330369   \n",
       "\n",
       "   card5_TransactionAmt_mean  card5_TransactionAmt_std  \n",
       "0                 141.865993                242.457293  \n",
       "1                 141.865993                242.457293  \n",
       "2                 141.865993                242.457293  \n",
       "3                  98.774960                141.059909  \n",
       "4                 124.389514                191.880900  \n",
       "\n",
       "[5 rows x 2160 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 506691 entries, 0 to 506690\n",
      "Columns: 2160 entries, TransactionID to card5_TransactionAmt_std\n",
      "dtypes: float64(410), int64(1750)\n",
      "memory usage: 8.2 GB\n"
     ]
    }
   ],
   "source": [
    "test_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating and Comparing Machine Learning Models\n",
    "In this section we will build, train, and evalute several machine learning methods for our supervised regression task. The objective is to determine which model holds the most promise for further development (such as hyperparameter tuning)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Not Splitting the Train and  Test set \n",
    "Usually, we would split the training dataset to continue feature selection and perform model evaluation. However, since Kaggle has provided us with a train and test set, we will move forward using these tables. \n",
    "\n",
    "Here is a reference for the train and test split labels:\n",
    "\n",
    "* X_train -> training table with target removed\n",
    "* Y_train -> target of training table\n",
    "* X_test -> test table with target removed\n",
    "* Y_test -> target of test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modify train and test table for modeling \n",
    "train_label = train_df['isFraud']\n",
    "train_df = train_df.drop(labels=['TransactionID','TransactionDT','isFraud'], axis=1)\n",
    "test_df = test_df.drop(labels=['TransactionID','TransactionDT'], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy datasets\n",
    "train = train_df.copy()\n",
    "test = test_df.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imputing Missing Values\n",
    "Standard machine learning models cannot deal with missing values, and which means we have to find a way to fill these in or disard any features with missing values. Here, we will fill in missing values with the mode of the column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tsawaengsri/anaconda3/lib/python3.6/site-packages/sklearn/utils/deprecation.py:58: DeprecationWarning: Class Imputer is deprecated; Imputer was deprecated in version 0.20 and will be removed in 0.22. Import impute.SimpleImputer from sklearn instead.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "# Create an imputer object with a most frequent filling strategy\n",
    "imputer = SimpleImputer(strategy='most_frequent')\n",
    "\n",
    "# Train on the training features\n",
    "imputer.fit(train)\n",
    "\n",
    "# Transform both training data and testing data\n",
    "train = imputer.transform(train)\n",
    "test = imputer.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values in training features:  0\n",
      "Missing values in testing features:   0\n"
     ]
    }
   ],
   "source": [
    "print('Missing values in training features: ', np.sum(np.isnan(train)))\n",
    "print('Missing values in testing features:  ', np.sum(np.isnan(test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([], dtype=int64), array([], dtype=int64))\n",
      "(array([], dtype=int64), array([], dtype=int64))\n"
     ]
    }
   ],
   "source": [
    "# Make sure all values are finite\n",
    "print(np.where(~np.isfinite(train)))\n",
    "print(np.where(~np.isfinite(test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaling Features \n",
    "The final step to take before we can build our models is to scale the features. This is necessary because features are in different units, and we want to normalize the features so the units do not affect the algorithm. Linear Regression and Random Forest do not require feature scaling, but other methods, such as support vector machines and k nearest neighbors, do require it because they take into account the Euclidean distance between observations. For this reason, it is a best practice to scale features when we are comparing multiple algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the scaler object with a range of 0-1\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "\n",
    "# Fit on the training data\n",
    "scaler.fit(train)\n",
    "\n",
    "# Transform both the training and testing data\n",
    "train = scaler.transform(train)\n",
    "test = scaler.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert train_label to one-dimensional array (vector)\n",
    "train_label = np.array(train_label).reshape((-1, ))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline \n",
    "For a naive baseline, logistic regression to predict the probability of occurrence of fraud by utilizing a logit function. \n",
    "\n",
    "## Logistic Regression \n",
    "First, we will use lasso regression to conduct feature selection.  first create the model and train the model using .fit.  Then, we make predictions on the testing data using .predict_proba (remember that we want probabilities and not a 0 or 1)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selecting features using Lasso regularisation\n",
    "\n",
    "Here I will do the model fitting and feature selection, altogether in one line of code. First I specify the Logistic Regression model, and I make sure I select the Lasso (L1) penalty.Then I use the selectFromModel object from sklearn, which will select in theory the features which coefficients are non-zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tsawaengsri/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SelectFromModel(estimator=LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "          n_jobs=None, penalty='l1', random_state=None, solver='warn',\n",
       "          tol=0.0001, verbose=0, warm_start=False),\n",
       "        max_features=None, norm_order=1, prefit=False, threshold=None)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sel_ = SelectFromModel(LogisticRegression(C=1, penalty='l1'))\n",
    "sel_.fit(scaler.transform(train), train_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Make a list of with the selected features.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'columns'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-b9de6eba4830>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mselected_feat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msel_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_support\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'total features: {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'selected features: {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mselected_feat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m print('features with coefficients shrank to zero: {}'.format(\n\u001b[1;32m      5\u001b[0m       np.sum(sel_.estimator_.coef_ == 0)))\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'columns'"
     ]
    }
   ],
   "source": [
    "selected_feat = train_df.columns[(sel_.get_support())]\n",
    "print('total features: {}'.format((train.shape[1])))\n",
    "print('selected features: {}'.format(len(selected_feat)))\n",
    "print('features with coefficients shrank to zero: {}'.format(\n",
    "      np.sum(sel_.estimator_.coef_ == 0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Identifying the removed features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'columns'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-d7e5a5dad4c3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mremoved_feats\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msel_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mestimator_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcoef_\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mremoved_feats\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'columns'"
     ]
    }
   ],
   "source": [
    "removed_feats = train_df.columns[(sel_.estimator_.coef_ == 0).ravel().tolist()]\n",
    "removed_feats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Removing the features from training an test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((590540, 780), (506691, 780))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_selected = sel_.transform(train)\n",
    "test_selected = sel_.transform(test)\n",
    "train_selected.shape, test_selected.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tsawaengsri/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# instantiate the model (using the default parameters)\n",
    "logreg = LogisticRegression()\n",
    "\n",
    "# fit the model with data\n",
    "logreg.fit(train_selected, train_label)\n",
    "\n",
    "#\n",
    "y_pred=logreg.predict(test_selected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [590540, 506691]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-1aca5870a39c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# ROC Curve\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0my_pred_proba\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogreg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_selected\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mfpr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtpr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroc_curve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_label\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0my_pred_proba\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mauc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroc_auc_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_label\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred_proba\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfpr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtpr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"data 1, auc=\"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mauc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/metrics/ranking.py\u001b[0m in \u001b[0;36mroc_curve\u001b[0;34m(y_true, y_score, pos_label, sample_weight, drop_intermediate)\u001b[0m\n\u001b[1;32m    616\u001b[0m     \"\"\"\n\u001b[1;32m    617\u001b[0m     fps, tps, thresholds = _binary_clf_curve(\n\u001b[0;32m--> 618\u001b[0;31m         y_true, y_score, pos_label=pos_label, sample_weight=sample_weight)\n\u001b[0m\u001b[1;32m    619\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    620\u001b[0m     \u001b[0;31m# Attempt to drop thresholds corresponding to points in between and\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/metrics/ranking.py\u001b[0m in \u001b[0;36m_binary_clf_curve\u001b[0;34m(y_true, y_score, pos_label, sample_weight)\u001b[0m\n\u001b[1;32m    397\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"{0} format is not supported\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    398\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 399\u001b[0;31m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    400\u001b[0m     \u001b[0my_true\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcolumn_or_1d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    401\u001b[0m     \u001b[0my_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcolumn_or_1d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_score\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_consistent_length\u001b[0;34m(*arrays)\u001b[0m\n\u001b[1;32m    233\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m         raise ValueError(\"Found input variables with inconsistent numbers of\"\n\u001b[0;32m--> 235\u001b[0;31m                          \" samples: %r\" % [int(l) for l in lengths])\n\u001b[0m\u001b[1;32m    236\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [590540, 506691]"
     ]
    }
   ],
   "source": [
    "# ROC Curve\n",
    "y_pred_proba = logreg.predict_proba(test_selected)[::,1]\n",
    "fpr, tpr, _ = metrics.roc_curve(train_label,  y_pred_proba)\n",
    "auc = metrics.roc_auc_score(train_label, y_pred_proba)\n",
    "plt.plot(fpr,tpr,label=\"data 1, auc=\"+str(auc))\n",
    "plt.legend(loc=4)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The logistic regression basline scored around 0.81. This model is slightly better than using only transaction dataset which scored 0.78."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models to Evaluate\n",
    "We will compare five different machine learning models:\n",
    "\n",
    "1. Logistic Regression\n",
    "2. Random Forest Classifer\n",
    "3. Support Vector Machine \n",
    "4. Gradient Boosting Classifer\n",
    "5. K-Nearest Neighbors Classifer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "def fit_and_evaluate(model):\n",
    "    kfold = KFold(n_splits=10, random_state=42)\n",
    "    cv_results = cross_val_score(model, train_selected, train_label, cv=kfold, scoring='roc_auc')\n",
    "    score = cv_results\n",
    "    name = str(model)\n",
    "    msg = '%s: %f (%f)' % (name, cv_results.mean(), cv_results.std())\n",
    "    print(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tsawaengsri/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/Users/tsawaengsri/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/Users/tsawaengsri/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/Users/tsawaengsri/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/Users/tsawaengsri/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/Users/tsawaengsri/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/Users/tsawaengsri/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/Users/tsawaengsri/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/Users/tsawaengsri/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/Users/tsawaengsri/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
      "          n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
      "          tol=0.0001, verbose=0, warm_start=False): 0.850680 (0.016009)\n"
     ]
    }
   ],
   "source": [
    "lr = LogisticRegression()\n",
    "fit_and_evaluate(lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tsawaengsri/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "/Users/tsawaengsri/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "/Users/tsawaengsri/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "/Users/tsawaengsri/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "/Users/tsawaengsri/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "/Users/tsawaengsri/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "/Users/tsawaengsri/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "/Users/tsawaengsri/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "/Users/tsawaengsri/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "/Users/tsawaengsri/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators='warn', n_jobs=None,\n",
      "            oob_score=False, random_state=None, verbose=0,\n",
      "            warm_start=False): 0.828824 (0.020985)\n"
     ]
    }
   ],
   "source": [
    "rfc = RandomForestClassifier()\n",
    "fit_and_evaluate(rfc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tsawaengsri/anaconda3/lib/python3.6/site-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "smv = SVC()\n",
    "fit_and_evaluate(smv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb = XGBClassifier()\n",
    "fit_and_evaluate(xgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNeighborsClassifier()\n",
    "fit_and_evaluate(knn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Improved Model: Random Forest\n",
    "Let's try using a Random Forest on the same training data to see if it will beat the performance of our baseline. The Random Forest is a much more powerful model especially when we use hundreds of trees. We will use 100 trees in the random forest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import Random Forest Model\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "#Create a Gaussian Classifier\n",
    "clf=RandomForestClassifier(n_estimators=100)\n",
    "\n",
    "#Train the model using the training sets y_pred=clf.predict(X_test)\n",
    "clf.fit(train_selected,train_label)\n",
    "\n",
    "y_pred=clf.predict(test_selected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC Curve\n",
    "y_pred_proba = clf.predict_proba(test_selected)[::,1]\n",
    "fpr, tpr, _ = metrics.roc_curve(train_label,  y_pred_proba)\n",
    "auc = metrics.roc_auc_score(train_label, y_pred_proba)\n",
    "plt.plot(fpr,tpr,label=\"data 1, auc=\"+str(auc))\n",
    "plt.legend(loc=4)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model score is around 0.957 which is 0.02 better than only using transaction dataset. This is also a drastic improvement from our baseline model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Interpretation: Feature Importances\n",
    "As a simple method to see which variables are the most relevant, we can look at the feature importances of the random forest. We may use these feature importances as a method of dimensionality reduction in future work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_imp = pd.Series(clf.feature_importances_).sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "# Creating a bar plot\n",
    "sns.barplot(x=feature_imp, y=feature_imp.index)\n",
    "# Add labels to your graph\n",
    "plt.xlabel('Feature Importance Score')\n",
    "plt.ylabel('Features')\n",
    "plt.title(\"Visualizing Important Features\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, this graph isn't helpful. How can I fix this? Maybe I'll try turning it into a list and return only the most top features. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion \n",
    "We followed the general outline of a machine learning project:\n",
    "\n",
    "1. Understand the problem and the data\n",
    "2. Data cleaning and formatting (this was mostly done for us)\n",
    "3. Exploratory Data Analysis\n",
    "4. Baseline model\n",
    "5. Improved model\n",
    "6. Model interpretation (just a little)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
