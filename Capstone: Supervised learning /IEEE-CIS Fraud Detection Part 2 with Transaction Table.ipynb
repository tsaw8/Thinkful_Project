{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IEEE-CIS Fraud Detection Part 2\n",
    "\n",
    "In this series of notebooks, we are working on a supervised, regression machine learning problem. Using Kaggle's competition [IEEE-CIS Fraud Detection](https://www.kaggle.com/c/ieee-fraud-detection) dataset, we want to predict whether a transaction is fraud or not. \n",
    "\n",
    " ### Workflow \n",
    " 1. Understand the problem (we're almost there already)\n",
    " 2. Exploratory Data Analysis\n",
    " 3. Feature engineering to create a dataset for machine learning\n",
    " 4. Create a baseline machine learning model\n",
    " 5. Try more complex machine learning models\n",
    " 6. Optimize the selected model\n",
    " 7. Investigate model predictions in context of problem\n",
    " 8. Draw conclusions and lay out next steps\n",
    " \n",
    "The first notebook covered steps 1-3, and in this notebook, we will cover 4-6."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read in Data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numpy and pandas\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "#Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import missingno as msno\n",
    "\n",
    "# Statistics tools\n",
    "import scipy.stats as stats\n",
    "\n",
    "# Sklearn data clean\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from dask_ml.impute import SimpleImputer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Model selection\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# Logistic Regression\n",
    "from dask_ml.linear_model import LogisticRegression\n",
    "\n",
    "# KNN Classifer \n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "## KNN Classifer \n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Decision Trees\n",
    "from sklearn import tree\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from IPython.display import Image\n",
    "import pydotplus\n",
    "import graphviz\n",
    "\n",
    "# Random Forests \n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# SVM\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Gradient Boost\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# Evaluate\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import f1_score,roc_auc_score, confusion_matrix, classification_report, accuracy_score\n",
    "\n",
    "# Hyperparameter tuning\n",
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n",
    "\n",
    "# Datetime\n",
    "from datetime import datetime\n",
    "\n",
    "# Import data\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Data\n",
    "df = pd.read_csv('/Users/tsawaengsri/Desktop/Data Science Courses/Datasets/ieee-fraud-detection/clean_train_trans.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display sizes of data\n",
    "print('Training Feature Size: ', df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['isFraud'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to memory limitations for model processing, we will use 30% (177,162 rows) of the data and randomly drop 413,378 rows. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop a random sample of 413,378 for testing rows\n",
    "df.drop(df.sample(413378).index, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.shape[0])\n",
    "df = df.sample(frac=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we contine, we will impute missing values, encode categorical variables, and scale features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imputing Missing Values \n",
    "Standard machine learning models cannot deal with missing values, and which means we have to find a way to fill these in or disard any features with missing values. Imputing also helps to reduce bias due to missingness: ‘rather than deleting cases that are subject to item-nonresponse, the sample size is maintained resulting in a potentially higher efficiency than for case deletion'[Durrant](https://www.tandfonline.com/doi/full/10.1080/1743727X.2014.979146#).\n",
    "\n",
    "Here, we will fill in missing values with the mean of the column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an imputer object with a mean filling strategy\n",
    "imputer = SimpleImputer(missing_values=np.NaN, strategy='mean')\n",
    "\n",
    "# Train on the training features\n",
    "imputer.fit(df)\n",
    "\n",
    "# Transform both training data and testing data\n",
    "df = imputer.transform(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Missing values in training set: ', np.sum(np.isnan(df)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding Categorical Variables\n",
    "Before we continue, most machine learning models cannot handle categorical variables well. Therefore, we will have to encode (represent) these variables as numbers before modeling. The two main encoding methods are: \n",
    " * __Label encoding__: assign each unique category in a categorical variable with an integer. No new columns are created.\n",
    " * __One-hot encoding__: create a new column for each unique category in a categorical variable. Each observation recieves a 1 in the column for its corresponding category and a 0 in all other new columns.\n",
    " \n",
    "The problem with label encoding is that it gives the categories an arbitrary ordering. The value assigned to each of the categories is random and does not reflect any inherent aspect of the category. \n",
    "\n",
    "For this project, we will use Label Encoding for any categorical variables for [tree-based models](https://roamanalytics.com/2016/10/28/are-categorical-variables-getting-lost-in-your-random-forests/) and One-Hot Encoding for any categorical variables for other models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a label encoder object\n",
    "le = LabelEncoder()\n",
    "le_count = 0\n",
    "\n",
    "df_le = df.copy()\n",
    "# Iterate through the columns\n",
    "for col in df_le:\n",
    "    if df_le[col].dtype == 'object':\n",
    "        # Train on the training data\n",
    "        le.fit(df_le[col])\n",
    "        # Transform both training and testing data\n",
    "        df_le[col] = le.transform(df_le[col])\n",
    "            \n",
    "        # Keep track of how many columns were label encoded\n",
    "        le_count += 1\n",
    "            \n",
    "print('%d columns were label encoded.' % le_count)\n",
    "print('Training Features shape: ', df_le.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encoding of categorical variables\n",
    "df_ = pd.get_dummies(df)\n",
    "\n",
    "print('Training Features shape: ', df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Train and Test set¶\n",
    "Let's split dataset by using function train_test_split(). Here, the Dataset is broken into two parts in a ratio of 80:20. It means 80% data will be used for model training and 20% for model testing.\n",
    "\n",
    "To continue feature selection, we will start by using the original attributes in the raw training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Y is the target variable\n",
    "y = df['isFraud']\n",
    "\n",
    "# X is the feature set\n",
    "X = df.drop(labels=['TransactionID','TransactionDT','isFraud'], axis=1).fillna(0) # Impute Nan with 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X_train.values\n",
    "X_test = X_test.values\n",
    "y = y_train.values\n",
    "y_test = y_test.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('X_shapes:\\n', 'X_train:', 'X_test:\\n', X.shape, X_test.shape, '\\n')\n",
    "print('Y_shapes:\\n', 'Y_train:', 'Y_test:\\n', y.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaling Features\n",
    "The final step to take before we can build our models is to scale the features. This is necessary because features are in different units, and we want to normalize the features so the units do not affect the algorithm. Linear Regression and Random Forest do not require feature scaling, but other methods, such as support vector machines and k nearest neighbors, do require it because they take into account the Euclidean distance between observations. For this reason, it is a best practice to scale features when we are comparing multiple algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the scaler object with a range of 0-1\n",
    "#scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "\n",
    "# Fit on the training data\n",
    "#scaler.fit(X)\n",
    "\n",
    "# Transform both the training and testing data\n",
    "#X = scaler.transform(X)\n",
    "#X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert y to one-dimensional array (vector)\n",
    "#y = np.array(y_train).reshape((-1, ))\n",
    "#y_test = np.array(y_test).reshape((-1, ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating and Comparing Machine Learning Models\n",
    "In this section we will build, train, and evalute several machine learning methods for our supervised regression task. The objective is to determine which model holds the most promise for further development (such as hyperparameter tuning)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline \n",
    "For a naive baseline, we will use logistic regression to predict the probability of fraud occurrence. Unlike linear regression which gives continuous output, logistic regression provides a constant output in prediciting binary classes. If the probability 'p' is greater than 0.5, the data is labeled '1'. Probability less than 0.5 is labeled as '0'.\n",
    "\n",
    "## Logistic Regression Implementation\n",
    "First, we'll create the model and train the model and make predictions on the testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logisitic Regression \n",
    "start_time = datetime.now()\n",
    "\n",
    "# Instantiate the model (using the default parameters)\n",
    "logreg = LogisticRegression()\n",
    "\n",
    "# Fit the model with data\n",
    "logreg.fit(X, y)\n",
    "\n",
    "# Predict on test set\n",
    "y_pred=logreg.predict(X_test)\n",
    "\n",
    "# Compute ROC AUC score, accuracy score, confusion matrix, and classification report\n",
    "print('ROC AUC score: %0.4f' % roc_auc_score(y_test, y_pred))\n",
    "print('Accuracy score: %0.4f' % accuracy_score(y_test, y_pred))      \n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "end_time = datetime.now()\n",
    "print('\\nDuration: {}'.format(end_time - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the ROC AUC and accuracy scores are very different. This model has an accuracy score of 95.2%. The score is calculated by adding the true positive and true negative values of 111,580 and 856] and dividing by the total count of 118,108. In contrast, the ROC AUC score is close to a random guess score of 59.1%. From the confusion matrix, we failed to flag 2,286 transactions as fraud out of 111,580 fraudulent transactions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC Curve\n",
    "y_pred_proba = logreg.predict_proba(X_test)\n",
    "fpr, tpr, _ = metrics.roc_curve(y_test,  y_pred_proba)\n",
    "auc = metrics.roc_auc_score(y_test, y_pred_proba)\n",
    "plt.plot(fpr,tpr,label=\"data 1, auc=\"+str(auc))\n",
    "plt.legend(loc=4)\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The predicted probability of the logistic regression basline scored around 0.6 which is better than a random guess score of 0.5.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Improved Model: Random Forest\n",
    "Let's try using a Random Forest on the same training data to see if it will beat the performance of our baseline. The Random Forest is a much more powerful model especially when we use hundreds of trees. We will use 100 trees in the random forest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest Classifer\n",
    "start_time = datetime.now()\n",
    "\n",
    "#Create a Gaussian Classifier\n",
    "clf=RandomForestClassifier(n_estimators=100)\n",
    "\n",
    "#Train the model using the training sets y_pred=clf.predict(X_test)\n",
    "clf.fit(X, y)\n",
    "\n",
    "y_pred=clf.predict(X_test)\n",
    "\n",
    "# Compute ROC AUC score, accuracy score, confusion matrix, and classification report\n",
    "print('ROC AUC score: %0.4f' % roc_auc_score(y_test, y_pred))\n",
    "print('Accuracy score: %0.4f' % accuracy_score(y_test, y_pred))      \n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "end_time = datetime.now()\n",
    "print('\\nDuration: {}'.format(end_time - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ROC AUC and accuracy score for this model has increased from the baseline model indicating that random forest is a the better model for this data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC Curve\n",
    "y_pred_proba = logreg.predict_proba(X_test)\n",
    "fpr, tpr, _ = metrics.roc_curve(y_test,  y_pred_proba)\n",
    "auc = metrics.roc_auc_score(y_test, y_pred_proba)\n",
    "plt.plot(fpr,tpr,label=\"data 1, auc=\"+str(auc))\n",
    "plt.legend(loc=4)\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This graph looks exactly like the baseline model. I'm not sure if this kernel ran properly. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Interpretation: Feature Importances\n",
    "As a simple method to see which variables are the most relevant, we can look at the feature importances of the random forest. We may use these feature importances as a method of dimensionality reduction in future work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#'Feature_importances_' from random forest\n",
    "feature_importances = pd.DataFrame(clf.feature_importances_,\n",
    "                                   index = df.columns,\n",
    "                                    columns=['importance']).sort_values('importance', ascending=False)\n",
    "print((feature_importances).head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance = clf.feature_importances_\n",
    "\n",
    "# Make importances relative to max importance.\n",
    "feature_importance = 100.0 * (feature_importance / feature_importance.max())\n",
    "sorted_idx = np.argsort(feature_importance)\n",
    "pos = np.arange(sorted_idx.shape[0]) + .5\n",
    "\n",
    "plt.barh(pos, feature_importance[sorted_idx], align='center')\n",
    "plt.yticks(pos, train.columns[sorted_idx])\n",
    "plt.xlabel('Relative Importance')\n",
    "plt.title('Variable Importance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models to Evaluate\n",
    "We will compare five different machine learning models:\n",
    "\n",
    "1. Logistic Regression\n",
    "2. Random Forest Classifer\n",
    "3. K-Nearest Neighbors Classifer\n",
    "4. Support Vector Machine \n",
    "5. Extreme Gradient Boosting Classifer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate roc auc score\n",
    "def auc(y_test, y_pred_proba):\n",
    "    auc = metrics.roc_auc_score(y_test, y_pred_proba)\n",
    "    return auc\n",
    "\n",
    "# Takes in a model, trains the model, and evaluates the model on the test set\n",
    "def fit_and_evaluate(model):\n",
    "    \n",
    "   # with joblib.parallel_backend('dask'):\n",
    "        # Train the model\n",
    "        model.fit(X, y)\n",
    "    \n",
    "        # Make predictions and evalute\n",
    "        model_pred = model.predict(X_test)\n",
    "    \n",
    "        # Compute ROC AUC score, accuracy score, confusion matrix, and classification report\n",
    "        print('ROC AUC score: %0.4f' % roc_auc_score(y_test, model_pred))\n",
    "        print('Accuracy score: %0.4f' % accuracy_score(y_test, model_pred))      \n",
    "        print(confusion_matrix(y_test, model_pred))\n",
    "        print(classification_report(y_test, model_pred))\n",
    "    \n",
    "        # ROC Curve\n",
    "        y_pred_proba = logreg.predict_proba(X_test)\n",
    "        fpr, tpr, _ = metrics.roc_curve(y_test,  y_pred_proba)\n",
    "        auc = metrics.roc_auc_score(y_test, y_pred_proba)\n",
    "        plt.plot(fpr,tpr,label=\"data 1, auc=\"+str(auc))\n",
    "        plt.legend(loc=4)\n",
    "        plt.xlabel('False Positive Rate')\n",
    "        plt.ylabel('True Positive Rate')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logisitic Regression \n",
    "start_time = datetime.now()\n",
    "\n",
    "lr = LogisticRegression()\n",
    "fit_and_evaluate(lr)\n",
    "\n",
    "end_time = datetime.now()\n",
    "print('\\nDuration: {}'.format(end_time - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest Classifer\n",
    "start_time = datetime.now()\n",
    "\n",
    "rfc = RandomForestClassifier()\n",
    "fit_and_evaluate(rfc)\n",
    "\n",
    "end_time = datetime.now()\n",
    "print('\\nDuration: {}'.format(end_time - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KNN Classifer \n",
    "start_time = datetime.now()\n",
    "\n",
    "knn = KNeighborsClassifier()\n",
    "fit_and_evaluate(knn)\n",
    "\n",
    "end_time = datetime.now()\n",
    "print('\\nDuration: {}'.format(end_time - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Support Vector Classifer\n",
    "start_time = datetime.now()\n",
    "\n",
    "smv = SVC()\n",
    "fit_and_evaluate(smv)\n",
    "\n",
    "end_time = datetime.now()\n",
    "print('\\nDuration: {}'.format(end_time - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extreme Gradient Boosting Classifer \n",
    "start_time = datetime.now()\n",
    "\n",
    "xgb = XGBClassifier()\n",
    "fit_and_evaluate(xgb)\n",
    "\n",
    "end_time = datetime.now()\n",
    "print('\\nDuration: {}'.format(end_time - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataframe to hold the results\n",
    "model_comparison = pd.DataFrame({'Model': ['Logistic Regression', 'RandomForest Classifier',\n",
    "                                           'Support Vector Classifer', 'KNeighbors Classifier', 'XGB Classifier'],\n",
    "                                 'ROC_AUC': [lr_score, rfc_score, svc_score, knn_score, xgb_score]})\n",
    "\n",
    "# Horizontal bar chart of test mae\n",
    "model_comparison.sort_values('ROC_AUC', ascending = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion \n",
    "We followed the general outline of a machine learning project:\n",
    "\n",
    "1. Understand the problem and the data\n",
    "2. Data cleaning and formatting (this was mostly done for us)\n",
    "3. Exploratory Data Analysis\n",
    "4. Baseline model\n",
    "5. Improved model\n",
    "6. Model interpretation (just a little)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
