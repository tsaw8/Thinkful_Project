{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RFC_with_Spark_in_Batch_Mode.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tsaw8/Thinkful_Project/blob/master/RFC_with_Spark_in_Batch_Mode.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "rCwqd_Gdlx10"
      },
      "source": [
        "In this notebook, you'll learn the basics of working with Spark in batch mode to build a random forest classifier. Note that this notebook is intended to be run on Google Colaboratory."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "53Wvo-RpPg4e",
        "colab_type": "text"
      },
      "source": [
        "## Spark and Colaboratory setup\n",
        "\n",
        "First, there's some configration specific to running Spark on Colaboratory that we'll need to attend to. Run these cells to set everything up."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ahlaHUHYnGcg",
        "colab": {}
      },
      "source": [
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q http://apache.osuosl.org/spark/spark-2.4.4/spark-2.4.4-bin-hadoop2.7.tgz\n",
        "!tar xf spark-2.4.4-bin-hadoop2.7.tgz"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_tfZIAZ8Pg4n",
        "colab_type": "code",
        "outputId": "453d3181-89c1-467f-fedf-6dc97c9615c0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "# Install spark-related depdencies for Python\n",
        "!pip install -Uq findspark\n",
        "!pip install -Uq pyspark"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[K     |████████████████████████████████| 215.7MB 55kB/s \n",
            "\u001b[K     |████████████████████████████████| 204kB 50.4MB/s \n",
            "\u001b[?25h  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "9pxKUb5pnHix",
        "colab": {}
      },
      "source": [
        "# Set up required environment variables\n",
        "\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-2.4.4-bin-hadoop2.7\"\n",
        "\n",
        "import findspark\n",
        "findspark.init()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "516x-s2QPg4x",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "ba2a170c-a390-4506-bf5c-4bc892c3b4f8"
      },
      "source": [
        "# Point Colaboratory to Google Drive\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "cTDxREFdlx11"
      },
      "source": [
        "##  Import dependencies\n",
        "\n",
        "Next, we need to import the tools we'll need from PySpark. The imports below allow us to connect to the Spark server, load our data, clean it, and prepare, execute, and evaluate a model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "UBIeh7Eblx12",
        "colab": {}
      },
      "source": [
        "from pyspark import SparkContext\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.classification import RandomForestClassifier\n",
        "from pyspark.ml.feature import IndexToString, StringIndexer, VectorIndexer, VectorAssembler\n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "\n",
        "from pyspark.sql.functions import isnan, when, count, col"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "5PLkpx0slx15"
      },
      "source": [
        "## Set our constants\n",
        "\n",
        "Next, we create a set of constants that we can refer to throughout the notebook. These are values that the rest of our code needs to run, but that we might need to change at some point (for instance, if the location of our data changes). \n",
        "\n",
        "If you saved the relevant datasets in the folders suggested in the previous checkpoint (link), you can use the below code chunk as is. If you saved the datasets elsewhere on your Google Drive, modify the file path after the `My Drive` folder. \n",
        "\n",
        "Regardless of the exact file path, these datasets **must** be stored on Google Drive!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Mqs6LIRelx16",
        "colab": {}
      },
      "source": [
        "CSV_PATH = \"/content/gdrive/My Drive/Colab Datasets/UCI_HAR/allData.csv\" \n",
        "CSV_ACTIVITY_LABEL_PATH = \"/content/gdrive/My Drive/Colab Datasets/UCI_HAR/activity_labels.csv\"\n",
        "APP_NAME = \"UCI HAR Random Forest Example\"\n",
        "SPARK_URL = \"local[*]\"\n",
        "RANDOM_SEED = 141107\n",
        "TRAINING_DATA_RATIO = 0.8\n",
        "RF_NUM_TREES = 10\n",
        "RF_MAX_DEPTH = 4\n",
        "RF_NUM_BINS = 32"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "hh1r-3Ialx19"
      },
      "source": [
        "## Connect to the server and load data\n",
        "\n",
        "Now we're ready to connect to the Spark server. We do that (relying on the constants set above) and then load our labels (loaded into `activity_labels`) and activity data (loaded into `df`). "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "e3daglotlx19",
        "colab": {}
      },
      "source": [
        "spark = SparkSession.builder.appName(APP_NAME).master(SPARK_URL).getOrCreate()\n",
        "activity_labels = spark.read.options(inferschema = \"true\").csv(CSV_ACTIVITY_LABEL_PATH)\n",
        "df = spark.read.options(inferschema = \"true\").csv(CSV_PATH)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "c-r3VQzLlx2A"
      },
      "source": [
        "## Validate the data\n",
        "\n",
        "If our data has been properly cleaned and prepared, it will meet the following criteria, which we'll verify in just a moment:\n",
        "\n",
        "* The dataframe shape should be 10,299 rows by 562 columns\n",
        "* All feature columns should be doubles. Note that one of the columns is for our labels and it will not be double.\n",
        "* There should be no nulls. This point is crucial because Spark will fail to build our vector variables for our classifier if there are any null values.\n",
        "\n",
        "Let's confirm these points."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "iokDZIFRlx2A",
        "outputId": "bb55ab2e-4c0b-4519-987a-c1347aaf9fa4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Confirm the dataframe shape is 10,299 rows by 562 columns\n",
        "print(f\"Dataset shape is {df.count():d} rows by {len(df.columns):d} columns.\")"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Dataset shape is 10299 rows by 562 columns.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Zh9sVIqtlx2E",
        "outputId": "18f66f8a-e7df-4231-9eb1-1730bb9ef02f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Confirm that all feature columns are doubles via a list comprehension\n",
        "# We're expecting 561 of 562 here, accounting for the labels column\n",
        "double_cols = [col[0] for col in df.dtypes if col[1] == 'double']\n",
        "print(f\"{len(double_cols):d} columns out of {len(df.columns):d} total are type double.\")"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "561 columns out of 562 total are type double.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "YEA2XsDtlx2I",
        "outputId": "8d2de0ee-e66a-4f61-b62b-8fea431ea388",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Confirm there are no null values. We use the dataframe select method to build a \n",
        "# list that is then converted to a Python dict. This way it's easy to sum up the nulls.\n",
        "null_counts = df.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) \n",
        "                         for c in df.columns]).toPandas().to_dict(orient='records')\n",
        "\n",
        "print(f\"There are {sum(null_counts[0].values()):d} null values in the dataset.\")"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "There are 0 null values in the dataset.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "vxdcUvn7lx2L"
      },
      "source": [
        "## Set up and run our classifier in Spark\n",
        "\n",
        "After confirming our data is clean, we're ready to reshape the data and run the random forest model.\n",
        "\n",
        "In Spark, we manipulate the data to work in a Spark pipeline, define each of the steps in the pipeline, chain them together, and finally run the pipeline.\n",
        "\n",
        "Apache Spark classifiers expect 2 columns of input:\n",
        "\n",
        "1. __labels__: an indexed set of numeric variables that represent the classification from the set of features we provide.\n",
        "2. __features__: an indexed, vector variable that contains all of the feature values in each row. \n",
        "\n",
        "In order to do this, we need to create these 2 columns from our dataset - the data is there, but not yet in a format we can use for the classifier.\n",
        "\n",
        "To create the indexed labels column, we'll create a column called `indexedLabel` using the `StringIndexer` method. We use the column `_c0` as the source for our label index since that contains our labels. The column contains only one value per index.\n",
        "    \n",
        "To create the indexed features column, we'll need to do two things. First, we'll create the vector of features using the `VectorAssembler` method. To create this vector, we'll need to use all 561 numeric columns from our data frame. The vector assembler will create a new column called `features`, and each row of this column will contain a 561-element vector that is built from the 561 features in the dataset.\n",
        "\n",
        "Finally, we'll complete the data preparation by creating an indexed vector from the `features` column. We'll call this vector `indexedFeatures`.\n",
        "    \n",
        "Since the classifier expects indexed labels and an indexed vector column of data, we'll use the `indexedLabel` and `indexedFeatures` as inputs to our random forest classifier."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "LlRqCnRslx2M",
        "colab": {}
      },
      "source": [
        "# Generate our feature vector.\n",
        "# Note that we're doing the work on the `df` object - we don't create new dataframes, \n",
        "# just add columns to the one we already are using.\n",
        "\n",
        "# the transform method creates the column.\n",
        "\n",
        "df = VectorAssembler(inputCols=double_cols, outputCol=\"features\").transform(df)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "GHNhdDitlx2O"
      },
      "source": [
        "Let's confirm that the features are there. It's easy to do this in Apache Spark using the `select` and `show` methods on the dataframe.  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "cFL8zXBDlx2P",
        "outputId": "9b6008f0-41eb-45c7-dfcc-b3dfedd7113e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "df.select(\"_c0\", \"features\").show(5)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---+--------------------+\n",
            "|_c0|            features|\n",
            "+---+--------------------+\n",
            "|  5|[0.289,-0.0203,-0...|\n",
            "|  5|[0.278,-0.0164,-0...|\n",
            "|  5|[0.28,-0.0195,-0....|\n",
            "|  5|[0.279,-0.0262,-0...|\n",
            "|  5|[0.277,-0.0166,-0...|\n",
            "+---+--------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "7JuLglvQlx2S"
      },
      "source": [
        "Now we're ready to build the indexers, split our data for training and testing, define our model, and finally chain everything together into a pipeline.\n",
        "\n",
        "__It's important to note that when we execute this cell, we're not actually running our model. At this point, we're only defining its parameters__."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "0tMHGAVylx2U",
        "colab": {}
      },
      "source": [
        "# Build the training indexers / split data / classifier\n",
        "# first we'll generate a labelIndexer\n",
        "labelIndexer = StringIndexer(inputCol=\"_c0\", outputCol=\"indexedLabel\").fit(df)\n",
        "\n",
        "# now generate the indexed feature vector\n",
        "featureIndexer = VectorIndexer(inputCol=\"features\", outputCol=\"indexedFeatures\", maxCategories=4).fit(df)\n",
        "    \n",
        "# Split the data into training and validation sets (30% held out for testing)\n",
        "(trainingData, testData) = df.randomSplit([TRAINING_DATA_RATIO, 1 - TRAINING_DATA_RATIO])\n",
        "\n",
        "# Train a RandomForest model.\n",
        "rf = RandomForestClassifier(labelCol=\"indexedLabel\", featuresCol=\"indexedFeatures\", numTrees=RF_NUM_TREES)\n",
        "\n",
        "# Chain indexers and forest in a Pipeline\n",
        "pipeline = Pipeline(stages=[labelIndexer, featureIndexer, rf])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "xBr2bivRlx2W"
      },
      "source": [
        "This next cell runs the pipeline, delivering a trained model at the end of the process."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "PVl2IQMolx2X",
        "scrolled": true,
        "colab": {}
      },
      "source": [
        "# Train model.  This also runs the indexers.\n",
        "model = pipeline.fit(trainingData)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "yBOhNbXPlx2Y"
      },
      "source": [
        "It is now easy to test our model and make predictions simply by using the model's `transform` method on the `testData` dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "AOpXWFSPlx2Z",
        "colab": {}
      },
      "source": [
        "# Make predictions.\n",
        "predictions = model.transform(testData)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "4ipzFtKllx2c"
      },
      "source": [
        "## Evaluate the model\n",
        "\n",
        "Now we can use the MulticlassClassificationEvaluator to test the model's accuracy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "mBGcd3x_lx2d",
        "outputId": "b997147e-1fca-4f2f-be08-867c6ef3861f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# Select (prediction, true label) and compute test error\n",
        "evaluator = MulticlassClassificationEvaluator(\n",
        "    labelCol=\"indexedLabel\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
        "accuracy = evaluator.evaluate(predictions)\n",
        "\n",
        "print(f\"Test Error = {(1.0 - accuracy):g}\")\n",
        "print(f\"Accuracy = {accuracy:g}\")"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test Error = 0.0768468\n",
            "Accuracy = 0.923153\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "pDrdglv-lx2h"
      },
      "source": [
        "## Next Steps\n",
        "\n",
        "We've seen how to prepare data and build a classifier in Spark. You might want to play around with this notebook and learn more about how Spark works. Here are some ideas:\n",
        "\n",
        "- Look at the set of labels, and see if there are any features that would make sense to combine. Spark allows you to map values into a new column.\n",
        "- Identify the most important features among the 561 source features (using PCA or something similar), then reduce the feature set and see if the model performs better.\n",
        "- Modify the settings of the random forest to see if the performance improves.\n",
        "- Use Spark's tools to find other techniques to evaluate the performance of your model. See if you can figure out how to generate an ROC plot, find the AUC value, or plot a confusion matrix."
      ]
    }
  ]
}