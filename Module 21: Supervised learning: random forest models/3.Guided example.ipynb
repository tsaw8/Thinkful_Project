{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forests \n",
    "\n",
    "Random forests is a supervised learning algorithm that is comprised of decision trees which are created from randomly selected data samples. The algorithm gets prediciton from each tree and selects the best solution by votes. The prediction result with the most votes becomes the final prediction. \n",
    "\n",
    "__Ensemble learning__ (or \"ensembling\") is simply the process of combining several models to solve a prediction problem, with the goal of producing a combined model that is more accurate than any individual model. For __classification__ problems, the combination is often done by majority vote. For __regression__ problems, the combination is often done by taking an average of the predictions. \n",
    "\n",
    "One popular method is __bootstrap aggregration/bagging__ where we take a subset of the data and train a model on each subset. Then the subsets are allowed to simultaneously vote on the outcome. This increases predictive accuracy by reducing the variance, similar to how cross-validation reduces the variance associated with the test set approach (for estimating out-of-sample error) by splitting many times an averaging the results.\n",
    "\n",
    "Rather than building muiltple models, __boosting__ uses the output of one model as an input into the next forming a a serial/daisy-chained process. \n",
    "\n",
    "The last category is __stacking__, which incorperates bagging and boosting. In the first phase, multiple models are trained in parallel. Then, those models are used as inputs into a final model to give a prediction. \n",
    "\n",
    "### Advantages\n",
    "* Random forests is considered as a highly accurate and robust method because of the number of decision trees participating in the process.\n",
    "* It does not suffer from the overfitting problem. The main reason is that it takes the average of all the predictions, which cancels out the biases.\n",
    "* The algorithm can be used in both classification and regression problems.\n",
    "* Random forests can also handle missing values by using median values to replace continuous variables, and computing the proximity-weighted average of missing values.\n",
    "\n",
    "### Disadvantages \n",
    "* Random forests is slow in generating predictions because it has multiple decision trees. \n",
    "* The model is difficult to interpret compared to a decision tree, where you can easily make a decision by following the path in the tree.\n",
    "\n",
    "### Finding important features \n",
    "Random forests also offer good feature selection indictors by showing relative importance of each feature in a prediction. It uses gini index to describe the explanatory power of a the variable. If the decrease of impurity is large after the binary split, then the variable is signigicant. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Example \n",
    "We will be building a model on the [Lending Club](https://www.lendingclub.com/info/download-data.action) 2015 dataset to predict the state of a loan given. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA \n",
    "from sklearn import ensemble\n",
    "from sklearn.model_selection import cross_val_score\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 824736\r\n",
      "drwxr-xr-x   9 tsawaengsri  staff   288B Aug 22 19:43 \u001b[34m.\u001b[m\u001b[m\r\n",
      "drwxr-xr-x  16 tsawaengsri  staff   512B Aug 20 14:34 \u001b[34m..\u001b[m\u001b[m\r\n",
      "-rw-r--r--@  1 tsawaengsri  staff   6.0K Aug 22 18:18 .DS_Store\r\n",
      "drwxr-xr-x   5 tsawaengsri  staff   160B Aug 21 16:57 \u001b[34m.ipynb_checkpoints\u001b[m\u001b[m\r\n",
      "-rw-r--r--   1 tsawaengsri  staff   208K Aug 20 16:10 1.Decision trees.ipynb\r\n",
      "-rw-r--r--   1 tsawaengsri  staff   3.2K Aug 21 00:21 2.The id3 algorithm.ipynb\r\n",
      "-rw-r--r--   1 tsawaengsri  staff    16K Aug 22 19:43 3.Guided example.ipynb\r\n",
      "-rw-r--r--   1 tsawaengsri  staff   357M Aug 21 17:58 LoanStats3d_securev1.csv\r\n",
      "-rw-r--r--   1 tsawaengsri  staff    45M Aug 22 19:45 my_beautiful_compressed_file.csv.xz\r\n"
     ]
    }
   ],
   "source": [
    "!ls -lha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tsawaengsri/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:5: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support skipfooter; you can avoid this warning by specifying engine='python'.\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "# Import Data\n",
    "yr2015 = pd.read_csv('my_beautiful_compressed_file.csv.xz')\n",
    "    # 'LoanStats3d_securev1.csv',\n",
    "                    #skipinitialspace=True,\n",
    "                    # header=1,\n",
    "                    # skipfooter=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "yr2015.to_csv('my_beautiful_compressed_file.csv.xz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yr2015.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like there are many rows with missing data, but that is ok since random forests can work with that. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yr2015.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since there are 150 attributes in this dataset, let's start to determine our model features by exploring the categorical data first. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning\n",
    "\n",
    "When selecting categorical variables for our model, we will use get_dummy function, which is memory intensive if there are many stinctive values. To reduce the complexity of our model, we will take a look at all our categorical variables and convert those with over 30 distinctive values to numeric values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical = yr2015.select_dtypes(include=['object'])\n",
    "for i in categorical:\n",
    "    column = categorical[i]\n",
    "    print(i)\n",
    "    print(column.nunique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a couple of columns, such as emp_title and revol_util that have more than a thousand distinctive values. Lets drop the ones with over 30 unique values, converting to numeric where it makes sense. In doing this there's a lot of code that gets written to just see if the numeric conversion makes sense. It's a manual process that we'll abstract away and just include the conversion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert ID and Interest Rate to numeric.\n",
    "yr2015['id'] = pd.to_numeric(yr2015['id'], errors='coerce')\n",
    "yr2015['int_rate'] = pd.to_numeric(yr2015['int_rate'].str.strip('%'), errors='coerce')\n",
    "\n",
    "# Drop other columns with many unique variables\n",
    "yr2015.drop(['url', 'emp_title', 'zip_code', 'earliest_cr_line', 'revol_util',\n",
    "            'sub_grade', 'addr_state', 'desc','last_pymnt_d','last_credit_pull_d',\n",
    "            'hardship_end_date','payment_plan_start_date','debt_settlement_flag_date'], 1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.get_dummies(yr2015)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iteration 1\n",
    "\n",
    "We will run the random forest classifier with all numeric and some categorical variables that have distinctive values less than 30. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiating the model\n",
    "\n",
    "rfc = ensemble.RandomForestClassifier()\n",
    "\n",
    "X = yr2015.drop('loan_status', 1)\n",
    "X = pd.get_dummies(X)\n",
    "# Dropping NA instead of imputing because data is probably rich enough\n",
    "X = X.dropna(axis=1)\n",
    "Y = yr2015['loan_status']\n",
    "\n",
    "cross_val_score(rfc, X, Y, cv=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The score cross validation reports is the accuracy of the tree. Here we're about 99% accurate.\n",
    "\n",
    "However, we did not refine the model so there maybe a few potential problems. Let's try to trim down as much data as possible without dropping below an average of 90% accuracy in a 10-fold cross validation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iteration 2\n",
    "\n",
    "Let's try to identify features with the most gini importance and use those variables as features. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In all feature selection procedures, it is a good practice to select the features by examining only the training set. This is to avoid overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the data into 20% test and 80% training\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train a Random Forest Classifer \n",
    "\n",
    "Here I will do the model fitting and feature selection altogether in one line of code.\n",
    "* Firstly, I specify the random forest instance, indicating the number of trees.\n",
    "* Then I use selectFromModel object from sklearn to automatically select the features.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Identify and Select Most Important Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc1_fi = rfc.feature_importances_\n",
    "indicies = np.argsort(rfc1_fi)\n",
    "feat_names = X.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to print the name and gini importance of each feature\n",
    "def feat_importance(feat_names, model):\n",
    "    for feature in zip(feat_names, model.feature_importances_):\n",
    "        print(feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_importance(feat_names, rfc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectFromModel\n",
    "# Create a selector object that will use the random forest classifier to identify\n",
    "# features that have an importance of more than 0.01\n",
    "sfm = SelectFromModel(rfc, threshold=0.01)\n",
    "\n",
    "# Train the selector\n",
    "sfm.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the names of the most important features\n",
    "for feature_list_index in sfm.get_support(indices=True):\n",
    "    print(feat_names[feature_list_index])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use the base features into the next model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create A Data Subset With Only The Most Important Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform the data to create a new dataset containing only the most important features\n",
    "# Note: We have to apply the transform to both the training X and test X data.\n",
    "X_important_train = sfm.transform(X_train)\n",
    "X_important_test = sfm.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train A New Random Forest Classifier Using Only Most Important Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new random forest classifier for the most important features\n",
    "\n",
    "cross_val_score(rfc, X_important_train, y_train, cv=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There wasn't much of a change from our first iteration. Let's try using only the top 5 columns. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From the top 5 features \n",
    "feature_cols = yr2015.loc[:,['funded_amnt','installment','out_prncp','out_prncp_inv',\n",
    "'total_pymnt']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = pd.get_dummies(feature_cols)\n",
    "x1 = x1.dropna(axis=1)\n",
    "y1 = Y\n",
    "\n",
    "rfc1 = ensemble.RandomForestClassifier()\n",
    "\n",
    "cross_val_score(rfc1, x1, y1, cv=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Those scores are still relatively high. Let's try to combine some features with PCA. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import bisect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_pca(df, expl_var=.95):\n",
    "    pca = PCA()\n",
    "    df = df.copy()\n",
    "    df = (df-df.mean())/df.std(ddof=0)\n",
    "    pca.fit(df)\n",
    "    varexp = pca.explained_variance_ratio_.cumsum()\n",
    "    cutoff = bisect.bisect(varexp, expl_var)\n",
    "    newcols = pd.DataFrame(pca.transform(df)[:, :cutoff+1], columns=['PCA'+df.columns[i] for i in range(cutoff+1)])\n",
    "    return pca, newcols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca, new_df = train_pca(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, maybe let's go back to dropping more columns. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From the top 3 features \n",
    "feature_cols2 = yr2015.loc[:,['funded_amnt','installment','out_prncp']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x2 = pd.get_dummies(feature_cols2)\n",
    "x2 = x2.dropna(axis=1)\n",
    "y1 = Y\n",
    "\n",
    "rfc1 = ensemble.RandomForestClassifier()\n",
    "\n",
    "cross_val_score(rfc1, x2, y1, cv=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These scores are too low. We'll revisit later. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
