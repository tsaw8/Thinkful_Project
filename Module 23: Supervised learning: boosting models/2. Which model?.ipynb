{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenge\n",
    "\n",
    "In this challenge, we will identify which supervised learning method(s) would be best for addressing that particular problem. First, let's compile a list of all the methods we learned so far. \n",
    "\n",
    "#### Naive Bayes \n",
    "Assumes independence between every pair of features and work well in many real-world situations such as document classification and spam filtering\n",
    " - __Advantages__ \n",
    "     -  requires a small amount of training data to estimate the necessary parameters\n",
    "     - extremely fast compared to more sophisticated methods\n",
    " - __Disadvantages__\n",
    "     - bad estimator \n",
    "\n",
    "#### Linear Regression \n",
    "Used to predictions problems, it find the target variable by finding a best suitable fit line between the independent and dependent variables\n",
    " - __Advantages__ \n",
    "     -  the best fit line is the line with minimum error from all the points\n",
    " - __Disadvantages__\n",
    "     - linear regression is limited to linear relationship\n",
    "     - only looks at the mean of the dependent variable\n",
    "     - sensitive to outliers\n",
    "     - data must be independent \n",
    "     \n",
    "#### KNN\n",
    "A type of lazy learning as it does not attempt to construct a general internal model, but simply stores instances of the training data. Classification is computed from a simple majority vote of the k nearest neighbours of each point.\n",
    " - __Advantages__ \n",
    "     -  simple to implement, robust to noisy training data, and effective if training data is large\n",
    " - __Disadvantages__\n",
    "     - need to determine the value of K and the computation cost is high as it needs to computer the distance of each instance to all the training samples\n",
    "     \n",
    "#### Decision Tree\n",
    "Given a data of attributes together with its classes, a decision tree produces a sequence of rules that can be used to classify the data\n",
    " - __Advantages__ \n",
    "     -  simple to understand and visualise\n",
    "     - requires little data preparation\n",
    "     - can handle both numerical and categorical data\n",
    " - __Disadvantages__\n",
    "     - can create complex trees that do not generalise well\n",
    "     - can be unstable because small variations in the data might result in a completely different tree being generated\n",
    "     \n",
    "#### Random Forest\n",
    "A meta-estimator that fits a number of decision trees on various sub-samples of datasets and uses average to improve the predictive accuracy of the model and controls over-fitting. The sub-sample size is always the same as the original input sample size but the samples are drawn with replacement.\n",
    " - __Advantages__ \n",
    "     - reduction in over-fitting and random forest classifier is more accurate than decision trees in most cases.\n",
    "\n",
    " - __Disadvantages__\n",
    "     - slow real time prediction\n",
    "     - difficult to implement\n",
    "     - complex algorithm\n",
    "     \n",
    "#### Support Vector Machine\n",
    "Representation of the training data as points in space separated into categories by a clear gap that is as wide as possible. New examples are then mapped into that same space and predicted to belong to a category based on which side of the gap they fall.\n",
    " - __Advantages__ \n",
    "     - effective in high dimensional spaces \n",
    "     - uses a subset of training points in the decision function so it is also memory efficient\n",
    "\n",
    " - __Disadvantages__\n",
    "     - does not directly provide probability estimates, these are calculated using an expensive five-fold cross-validation\n",
    "     \n",
    "#### Gradient Boosting\n",
    "Build an ensemble of shallow and weak successive trees with each tree learning and improving on the previous. When combined, these many weak successive trees produce a powerful “committee” that are often hard to beat with other algorithms\n",
    " - __Advantages__ \n",
    "     - high predictive accuracy\n",
    "     - flexibility, can optimize on different loss function and provides several hyperparameter tuning options\n",
    "     - no data pre-processing required, great with categorical and numerical values\n",
    "\n",
    " - __Disadvantages__\n",
    "     - can overemphasize outliers and cause overfitting, must use cross validation \n",
    "     - time and memory tensive \n",
    "     - high flexibility requires tunning \n",
    "     - less interpertable "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Predict the running times of prospective Olympic sprinters using data from the last 20 Olympics.\n",
    "We would use a regression model to predict running times. Each sprinter would be considered as an observation, qualities of the sprinter would be features, and the Olympic running time would be the outcome variables. \n",
    "\n",
    "Assuming that there are more than 50 observations and a few important features have been selected, we can use either Lasso or ElasticNet regression. However, if we wanted to use multiple features for each sprinter, we would need something more memory intensive such as a random forest."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. You have more features (columns) than rows in your dataset.\n",
    "We could use linear regression for this problem. However, we would have to regularize the estimated coefficients and shrink them to almost zero to avoid over fitting the data. In ridge regression, a tuning parameter is added to the cost function to penalize large coefficients. \n",
    "\n",
    "If this was a classification problem, Naive Bayes has an advantage with small training sets since its low bias/high variance. The latter tends to not over fit.\n",
    "\n",
    "Using PCA to reduce dimensions and random noise could also be helpful."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Identify the most important characteristic predicting likelihood of being jailed before age 20.\n",
    "\n",
    "Since decision trees are easy to interpret, we could create one to see feature interactions. Another alterative would be to ensemble random forests and return a list of feature importance. We could also identify which coefficients are statistically significant to the outcome in an OLS summary chart."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Implement a filter to “highlight” emails that might be important to the recipient\n",
    "Naive Bayes is great in classifying with very large datasets. SVMs also have high accuracy and work well with text classification problems. It can deal with non-linear data and high-dimensional space."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. You have 1000+ features.\n",
    "\n",
    "Before selecting a model, running PCA during feature engineering would be a good idea to combine collinear features and reduce random noise in the data. \n",
    "\n",
    "Lasso regression could be used to shrink the coefficients of unwanted features to zero. We could also use R-squared from Linear regression to keep only statistically significant variables. \n",
    "\n",
    "Random forests and SVMs would be also be a great choice to handle over fitting for this problem since there are so many features."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. Predict whether someone who adds items to their cart on a website will purchase the items.\n",
    "We can view the outcome as binary classification in determining whether a customer will purchase added items in their cart or not. If variables are independent of each other and dataset is free of missing values, linear regression model would be a good start. With a large training data, we could use KNN."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7. Your dataset dimensions are 982400 x 500\n",
    "It would be best to run PCA to reduce dimensionality and employ other feature selection techniques to select most important features. Using random forest or SVM would be best due to the high dimension space of the dataset and possibility of over fitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8. Identify faces in an image.\n",
    "Due to the immense dimensions of this task, I would recommend using SVMs or PCA to process and identify geometric shapes that resembles a face. \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 9. Predict which of three flavors of ice cream will be most popular with boys vs girls.\n",
    "Since this a classification task with a few features, Navies Bayes, linear classification or KNN would do the trick.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
