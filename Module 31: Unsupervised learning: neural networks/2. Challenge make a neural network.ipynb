{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Context \n",
    "\n",
    "For this challenge, we will create a multi-layer perceptron neural network model to predict fraudulent transactions on Kaggle's competition [IEEE-CIS Fraud Detection](https://www.kaggle.com/c/ieee-fraud-detection) dataset. We will also compare this model to a random forest model and describe the relative tradeoffs between complexity and accuracy.\n",
    "\n",
    "## Data Overview \n",
    "The datasets contains historical Vesta's real-world e-commerce transaction. There is a high imbalance of classes since the positive class (frauds) only account for 3.95% of all transactions. To treat this issue, we have undersampled the dataset and reduced the amount of normal transactions to equal the amount of fraudulent transactions. \n",
    "\n",
    "Due to confidentiality issues, original features are masked, which presents a challenge when interpreting the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "# Datetime\n",
    "from datetime import datetime\n",
    "\n",
    "# Data Cleaning \n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Model Selection \n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Evaluate \n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Ensemble Model\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# SVM \n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Network \n",
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('/Users/tsawaengsri/Desktop/Data Science Courses/Datasets/ieee-fraud-detection/clean_df_le.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>isFraud</th>\n",
       "      <th>TransactionAmt</th>\n",
       "      <th>ProductCD</th>\n",
       "      <th>card1</th>\n",
       "      <th>card2</th>\n",
       "      <th>card3</th>\n",
       "      <th>card4</th>\n",
       "      <th>card5</th>\n",
       "      <th>card6</th>\n",
       "      <th>addr1</th>\n",
       "      <th>...</th>\n",
       "      <th>id_38</th>\n",
       "      <th>DeviceType</th>\n",
       "      <th>transaction_day_of_week</th>\n",
       "      <th>transaction_hour</th>\n",
       "      <th>average_trans_amt_for_card1</th>\n",
       "      <th>average_trans_amt_for_card4</th>\n",
       "      <th>average_id_02_for_card1</th>\n",
       "      <th>average_id_02_for_card4</th>\n",
       "      <th>P_major_email</th>\n",
       "      <th>R_major_email</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>107.950</td>\n",
       "      <td>4</td>\n",
       "      <td>12695</td>\n",
       "      <td>490.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>3</td>\n",
       "      <td>226.0</td>\n",
       "      <td>1</td>\n",
       "      <td>325.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.033636</td>\n",
       "      <td>0.970722</td>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>25.000</td>\n",
       "      <td>1</td>\n",
       "      <td>12929</td>\n",
       "      <td>285.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>3</td>\n",
       "      <td>226.0</td>\n",
       "      <td>1</td>\n",
       "      <td>184.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>1.160000</td>\n",
       "      <td>0.210830</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.207972</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>57.950</td>\n",
       "      <td>4</td>\n",
       "      <td>9500</td>\n",
       "      <td>321.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>3</td>\n",
       "      <td>226.0</td>\n",
       "      <td>1</td>\n",
       "      <td>204.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>0.396588</td>\n",
       "      <td>0.428929</td>\n",
       "      <td>1.006090</td>\n",
       "      <td>0.970722</td>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>100.000</td>\n",
       "      <td>1</td>\n",
       "      <td>12769</td>\n",
       "      <td>555.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>2</td>\n",
       "      <td>224.0</td>\n",
       "      <td>1</td>\n",
       "      <td>204.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.936085</td>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>32.356</td>\n",
       "      <td>0</td>\n",
       "      <td>12778</td>\n",
       "      <td>500.0</td>\n",
       "      <td>185.0</td>\n",
       "      <td>2</td>\n",
       "      <td>224.0</td>\n",
       "      <td>0</td>\n",
       "      <td>290.733794</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>1.090654</td>\n",
       "      <td>0.379163</td>\n",
       "      <td>3.187676</td>\n",
       "      <td>3.957320</td>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 422 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   isFraud  TransactionAmt  ProductCD  card1  card2  card3  card4  card5  \\\n",
       "0        0         107.950          4  12695  490.0  150.0      3  226.0   \n",
       "1        0          25.000          1  12929  285.0  150.0      3  226.0   \n",
       "2        0          57.950          4   9500  321.0  150.0      3  226.0   \n",
       "3        0         100.000          1  12769  555.0  150.0      2  224.0   \n",
       "4        1          32.356          0  12778  500.0  185.0      2  224.0   \n",
       "\n",
       "   card6       addr1  ...  id_38  DeviceType  transaction_day_of_week  \\\n",
       "0      1  325.000000  ...      0           0                      6.0   \n",
       "1      1  184.000000  ...      1           1                      1.0   \n",
       "2      1  204.000000  ...      0           0                      4.0   \n",
       "3      1  204.000000  ...      0           0                      5.0   \n",
       "4      0  290.733794  ...      0           0                      5.0   \n",
       "\n",
       "   transaction_hour  average_trans_amt_for_card1  average_trans_amt_for_card4  \\\n",
       "0              23.0                          NaN                          NaN   \n",
       "1              17.0                     1.160000                     0.210830   \n",
       "2              22.0                     0.396588                     0.428929   \n",
       "3              17.0                          NaN                          NaN   \n",
       "4              23.0                     1.090654                     0.379163   \n",
       "\n",
       "   average_id_02_for_card1  average_id_02_for_card4  P_major_email  \\\n",
       "0                 1.033636                 0.970722             15   \n",
       "1                 1.000000                 0.207972              1   \n",
       "2                 1.006090                 0.970722             15   \n",
       "3                 1.000000                 0.936085             15   \n",
       "4                 3.187676                 3.957320             15   \n",
       "\n",
       "   R_major_email  \n",
       "0             15  \n",
       "1              1  \n",
       "2             15  \n",
       "3             15  \n",
       "4             15  \n",
       "\n",
       "[5 rows x 422 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 32902 entries, 0 to 32901\n",
      "Columns: 422 entries, isFraud to R_major_email\n",
      "dtypes: float64(395), int64(27)\n",
      "memory usage: 105.9 MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    16451\n",
       "0    16451\n",
       "Name: isFraud, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['isFraud'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Train and Test setÂ¶\n",
    "Let's split dataset by using function train_test_split(). Here, the Dataset is broken into two parts in a ratio of 80:20. It means 80% data will be used for model training and 20% for model testing.\n",
    "\n",
    "To continue feature selection, we will start by using the original attributes in the raw training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X is the feature set\n",
    "X = df.drop(labels=['isFraud'], axis=1)\n",
    "\n",
    "# Y is the target variable\n",
    "y = df['isFraud']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.values\n",
    "X_test = X_test.values\n",
    "y_train = y_train.values\n",
    "y_test = y_test.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_shapes:\n",
      " X_train: X_test:\n",
      " (26321, 421) (6581, 421) \n",
      "\n",
      "Y_shapes:\n",
      " Y_train: Y_test:\n",
      " (26321,) (6581,)\n"
     ]
    }
   ],
   "source": [
    "print('X_shapes:\\n', 'X_train:', 'X_test:\\n', X_train.shape, X_test.shape, '\\n')\n",
    "print('Y_shapes:\\n', 'Y_train:', 'Y_test:\\n', y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imputing Missing Values Again\n",
    "Standard machine learning models cannot deal with missing values, and which means we have to find a way to fill these in or disard any features with missing values. Imputing also helps to reduce bias due to missingness: â€˜rather than deleting cases that are subject to item-nonresponse, the sample size is maintained resulting in a potentially higher efficiency than for case deletion'[Durrant](https://www.tandfonline.com/doi/full/10.1080/1743727X.2014.979146#).\n",
    "\n",
    "Here, we will fill in missing values with the mean of the column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an imputer object with a mean filling strategy\n",
    "imputer = SimpleImputer(missing_values=np.NaN, strategy='mean')\n",
    "\n",
    "# Train on the training features\n",
    "imputer.fit(X_train)\n",
    "\n",
    "# Transform both training data and testing data\n",
    "X_train = imputer.transform(X_train)\n",
    "X_test = imputer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values in training features:  0\n",
      "Missing values in testing features:   0\n"
     ]
    }
   ],
   "source": [
    "print('Missing values in training features: ', np.sum(np.isnan(X_train)))\n",
    "print('Missing values in testing features:  ', np.sum(np.isnan(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([], dtype=int64), array([], dtype=int64))\n",
      "(array([], dtype=int64), array([], dtype=int64))\n"
     ]
    }
   ],
   "source": [
    "# Make sure all values are finite\n",
    "print(np.where(~np.isfinite(X_train)))\n",
    "print(np.where(~np.isfinite(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Classifer\n",
      "[0.8317189  0.83833587 0.83985562 0.84004559 0.83453647]\n",
      "Accuracy: 0.8378665856252849\n",
      "[[2846  459]\n",
      " [ 608 2668]]\n",
      "\n",
      "Duration: 0:01:15.136509\n"
     ]
    }
   ],
   "source": [
    "# Random Forest Classifer\n",
    "start_time = datetime.now()\n",
    "\n",
    "print('Random Forest Classifer')\n",
    "\n",
    "#Create a Gaussian Classifier\n",
    "clf = RandomForestClassifier(n_estimators=100)\n",
    "\n",
    "#Train the model using the training sets y_pred=clf.predict(X_test)\n",
    "print(cross_val_score(clf, X_train, y_train, cv=5))\n",
    "\n",
    "clf.fit(X_train,y_train)\n",
    "y_pred=clf.predict(X_test)\n",
    "\n",
    "# Model Accuracy, how often is the classifier correct?\n",
    "print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "end_time = datetime.now()\n",
    "print('\\nDuration: {}'.format(end_time - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, the model misclassified 3.24% of transactions. 13.8% of normal transactions were labeled as fraud and 18.5% of fraudulent transactions were classifed as normal transactions. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Interpretation: Feature Importances\n",
    "For model interpretability, we will take a look at the feature importances of our initial random forest. We may use these feature importances as a method of dimensionality reduction in future work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Variable</th>\n",
       "      <th>Importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TransactionAmt</td>\n",
       "      <td>0.024504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>C13</td>\n",
       "      <td>0.022797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>C14</td>\n",
       "      <td>0.019286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>card1</td>\n",
       "      <td>0.019266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>417</th>\n",
       "      <td>average_id_02_for_card1</td>\n",
       "      <td>0.018177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>card2</td>\n",
       "      <td>0.017260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>415</th>\n",
       "      <td>average_trans_amt_for_card1</td>\n",
       "      <td>0.015980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>416</th>\n",
       "      <td>average_trans_amt_for_card4</td>\n",
       "      <td>0.015560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>C8</td>\n",
       "      <td>0.015486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>414</th>\n",
       "      <td>transaction_hour</td>\n",
       "      <td>0.014654</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        Variable  Importance\n",
       "0                 TransactionAmt    0.024504\n",
       "23                           C13    0.022797\n",
       "24                           C14    0.019286\n",
       "2                          card1    0.019266\n",
       "417      average_id_02_for_card1    0.018177\n",
       "3                          card2    0.017260\n",
       "415  average_trans_amt_for_card1    0.015980\n",
       "416  average_trans_amt_for_card4    0.015560\n",
       "18                            C8    0.015486\n",
       "414             transaction_hour    0.014654"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Top N importances\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "N = 10\n",
    "importances = clf.feature_importances_\n",
    "std = np.std([tree.feature_importances_ for tree in clf.estimators_],\n",
    "             axis=0)\n",
    "\n",
    "# Create a dataframe\n",
    "importances_df = pd.DataFrame({'Variable':X.columns, 'Importance': importances})\n",
    "\n",
    "top_N = importances_df.sort_values(by=['Importance'], ascending=False).head(10)\n",
    "\n",
    "top_N"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Support Vector Classifer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tsawaengsri/anaconda3/lib/python3.6/site-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "/Users/tsawaengsri/anaconda3/lib/python3.6/site-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "/Users/tsawaengsri/anaconda3/lib/python3.6/site-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "/Users/tsawaengsri/anaconda3/lib/python3.6/site-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "/Users/tsawaengsri/anaconda3/lib/python3.6/site-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.50047483 0.50056991 0.50056991 0.50056991 0.50056991]\n",
      "Accuracy: 0.8393861115332016\n",
      "[[2851  454]\n",
      " [ 603 2673]]\n",
      "\n",
      "Duration: 0:29:21.422980\n"
     ]
    }
   ],
   "source": [
    "# Support Vector Classifer\n",
    "start_time = datetime.now()\n",
    "\n",
    "print('Support Vector Classifer')\n",
    "\n",
    "#Create a Gaussian Classifier\n",
    "svc = SVC(C=1e-9, kernel='rbf')\n",
    "\n",
    "#Train the model using the training sets y_pred=clf.predict(X_test)\n",
    "print(cross_val_score(svc, X_train, y_train, cv=5))\n",
    "\n",
    "y_pred=clf.predict(X_test)\n",
    "\n",
    "# Model Accuracy, how often is the classifier correct?\n",
    "print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "end_time = datetime.now()\n",
    "print('\\nDuration: {}'.format(end_time - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, the model misclassified 3.21% of transactions. This is slightly better than the random forest model. 13.7% of normal transactions were labeled as fraud and 18.4% of fraudulent transactions were classifed as normal transactions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge 4.3.6 Make Your Network\n",
    "Create a multi-layer perceptron neural network model to predict on a labeled dataset of your choosing. Compare this model to either a boosted tree or a random forest model and describe the relative tradeoffs between complexity and accuracy. Be sure to vary the hyperparameters of your MLP!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.712738877702215"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Establish and fit the model, with a single, 1000 perceptron layer.\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(1000,))\n",
    "mlp.fit(X_train, y_train)\n",
    "\n",
    "mlp.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.73162393, 0.73347264, 0.72473404, 0.71162614, 0.64266717])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_val_score(mlp, X_train, y_train, cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3090,  215],\n",
       "       [1671, 1605]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's see how the testing data performs.\n",
    "mlp_pred = mlp.predict(X_test)\n",
    "\n",
    "confusion_matrix(y_test, mlp_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It doesn't seem like this model is able to perform better than SVM or random forest. Let's try adding hidden layers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7187036966680598"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Attempt more hidden layers. \n",
    "mlp2 = MLPClassifier(hidden_layer_sizes=(1000,))\n",
    "mlp2.fit(X_train, y_train)\n",
    "\n",
    "mlp2.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.629459367045325"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Attempt more hidden layers. \n",
    "mlp3 = MLPClassifier(hidden_layer_sizes=(1000,100))\n",
    "mlp3.fit(X_train, y_train)\n",
    "\n",
    "mlp3.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.509023213403746"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Change the activation method. \n",
    "mlp4 = MLPClassifier(hidden_layer_sizes=(1000,100), activation='logistic')\n",
    "mlp4.fit(X_train, y_train)\n",
    "\n",
    "mlp4.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.57948718, 0.56591945, 0.5949848 , 0.57408815, 0.57940729])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_val_score(mlp4, X_train, y_train, cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6009650089282322"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Increase the amount of iterations. \n",
    "mlp5 = MLPClassifier(hidden_layer_sizes=(1000,100,100), activation='logistic', max_iter=500)\n",
    "mlp5.fit(X_train, y_train)\n",
    "\n",
    "mlp5.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Increasing hidden layers and changing the activation method did not increase the model's performance. Since neural networks perform better with more data, maybe oversampling would improve the model's performance. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
