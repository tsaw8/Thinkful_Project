{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge\n",
    "\n",
    "Do a little scraping or API-calling of your own.  Pick a new website and see what you can get out of it.  Expect that you'll run into bugs and blind alleys, and rely on your mentor to help you get through.  \n",
    "\n",
    "Formally, your goal is to write a scraper that will:\n",
    "\n",
    "1) Return specific pieces of information (rather than just downloading a whole page)  \n",
    "2) Iterate over multiple pages/queries  \n",
    "3) Save the data to your computer  \n",
    "\n",
    "Once you have your data, compute some statistical summaries and/or visualizations that give you some new insights into your scraping topic of interest.  Write up a report from scraping code to summary and share it with your mentor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import scrapy \n",
    "import scrapy\n",
    "from scrapy.crawler import CrawlerProcess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ESSpider(scrapy.Spider):\n",
    "    # Name is important, need different ones for each spider\n",
    "    # of the same class\n",
    "    name = 'ESS'\n",
    "    \n",
    "    start_urls = ['http://www.everydaysexism.com']\n",
    "    \n",
    "    # Defining the scraping process\n",
    "    def parse(self, response):\n",
    "        with open('./scraper_results/mainpage.html', 'wb') as f:\n",
    "            f.write(response.body)\n",
    "\n",
    "# Instantiate the crawler\n",
    "process = CrawlerProcess()\n",
    "\n",
    "# Start the crawler with the spider\n",
    "process.crawl(ESSpider)\n",
    "process.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we have a file called 'mainpage.html' saved to your machine that contains all the code from www.everydaysexism.com. However, to get more useful, parsed data, we must give the spider more specific instructions.\n",
    "\n",
    "__Note:__ Remember, to restart the kernel if you want to rerun a Scrapy script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scrapy\n",
    "from scrapy.crawler import CrawlerProcess\n",
    "\n",
    "class ESSpider(scrapy.Spider):\n",
    "    name = 'ESS'\n",
    "    start_urls = ['http://www.everydaysexism.com']\n",
    "    \n",
    "    def parse(self, response):\n",
    "        # Iterate over every <article> element on the page\n",
    "        for article in response.xpath('//article'):\n",
    "            yield {\n",
    "                'name': article.xpath('header/h2/a/@title').extract_first(),\n",
    "                'date': article.xpath('header/section/span[@class=\"entry-date\"]/text()').extract_first(),\n",
    "                'text': article.xpath('section[@class=\"entry-content\"]/p/text()').extract(),\n",
    "                'tags': article.xpath('*/span[@class=\"tag-links\"]/a/text()').extract()\n",
    "            }\n",
    "            \n",
    "            \n",
    "# Pass in crawler parameters\n",
    "process = CrawlerProcess({\n",
    "    'FEED_FORMAT': 'json',         # Store data in json\n",
    "    'FEED_URI': './scraper_results/firstpage.json',  # Name of the json file\n",
    "    'LOG_ENABLED': False           # Turning off logging\n",
    "})\n",
    "\n",
    "# Start the crawler & spider\n",
    "process.crawl(ESSpider)\n",
    "process.start()\n",
    "print('Success')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "\n",
    "firstpage = pd.read_json('./scraper_results/firstpage.json', orient='records')\n",
    "print(firstpage.shape)\n",
    "firstpage.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recursion\n",
    "Now that we have a scraper that can pull the information we want off of a page and store it in a file, we want to run that scraper over all the pages of the website. We do this using recursion – the Scrapy spider will run over a page, gather information, and then detect a link to the next page and call itself on the new page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scrapy\n",
    "from scrapy.crawler import CrawlerProcess\n",
    "import re\n",
    "\n",
    "class ESSpider(scrapy.Spider):\n",
    "    name = 'ESS'\n",
    "    start_urls = ['http://www.everydaysexism.com']\n",
    "    \n",
    "    def parse(self, response):\n",
    "\n",
    "        # Iterate over every <article> element on the page\n",
    "        for article in response.xpath('//article'):\n",
    "            yield {\n",
    "                'name': article.xpath('header/h2/a/@title').extract_first(),\n",
    "                'date': article.xpath('header/section/span[@class=\"entry-date\"]/text()').extract_first(),\n",
    "                'text': article.xpath('section[@class=\"entry-content\"]/p/text()').extract(),\n",
    "                'tags': article.xpath('*/span[@class=\"tag-links\"]/a/text()').extract()\n",
    "            }\n",
    "\n",
    "            # Getting the next page URL\n",
    "            next_page = response.xpath('//div[@class=\"nav-previous\"]/a/@href').extract_first()\n",
    "\n",
    "            # Grabbing the next page number\n",
    "            pagenum = int(re.findall(r'\\d+', next_page)[0])\n",
    "\n",
    "            # Recursively call the spider until page 9\n",
    "            if next_page is not None and pagenum < 10:\n",
    "                next_page = response.urljoin(next_page)\n",
    "                # Request next page with same parsing as above\n",
    "                yield scrapy.Request(next_page, callback=self.parse)\n",
    "\n",
    "            \n",
    "# Pass in crawler parameters\n",
    "# Additional parameters are for scraping etiquette\n",
    "process = CrawlerProcess({\n",
    "    'FEED_FORMAT': 'json',\n",
    "    'FEED_URI': './scraper_results/data.json',\n",
    "    'LOG_ENABLED': False,\n",
    "    'ROBOTSTXT_OBEY': True,\n",
    "    'USER_AGENT': 'ThinkfulDataScienceBootcampCrawler (thinkful.com)',\n",
    "    'AUTOTHROTTLE_ENABLED': True,\n",
    "    'HTTPCACHE_ENABLED': True\n",
    "})\n",
    "\n",
    "# Instantiate and start crawler\n",
    "process.crawl(ESSpider)\n",
    "process.start()\n",
    "print('Success')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_json('./scraper_results/data.json', orient='records')\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nine pages at 10 entries a row gives us 90 rows - looks like we were successful in our scraping! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge\n",
    "\n",
    "Now, we will try to pull Nathaniel Rakich's articles from [FiveThirtyEight](https://fivethirtyeight.com/contributors/nathaniel-rakich/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success\n"
     ]
    }
   ],
   "source": [
    "import scrapy\n",
    "from scrapy.crawler import CrawlerProcess\n",
    "import re \n",
    "\n",
    "class FiveThirtyEight(scrapy.Spider):\n",
    "    name = \"NS\"\n",
    "    \n",
    "    start_urls = ['https://fivethirtyeight.com/contributors/nathaniel-rakich/']\n",
    "    \n",
    "    def parse(self, response):\n",
    "        for item in response.xpath(\"//div[@class='content-area']/div\"):\n",
    "            yield {\n",
    "                'date': item.xpath(\".//div[@class='post-info']/p/time/text()\").extract_first(),\n",
    "                'title': item.xpath(\".//div[@class='post-info']/div/div/h2/a/text()\").extract_first(),\n",
    "                'article_link': item.xpath(\".//div[@class='post-info']/div/div/h2/a/@href\").extract_first(),\n",
    "                'author': item.xpath(\".//div[@class='post-info']/div/div/p[@class='single-metadata card vcard']/a/text()\").extract_first()\n",
    "            }\n",
    "        \n",
    "        nextpage = response.xpath(\"//div[@class='links']/a/@href\").extract_first()\n",
    "        pagenum = int(re.findall(r'\\d+', nextpage)[0])\n",
    "        \n",
    "        # Recursively call next page\n",
    "        if nextpage is not None and pagenum < 4: \n",
    "            nextpage = response.urljoin(nextpage)\n",
    "            yield scrapy.Request(nextpage, callback=self.parse)\n",
    "            \n",
    "            \n",
    "# Passing crawler parameters\n",
    "process = CrawlerProcess({\n",
    "    'FEED_FORMAT': 'json',\n",
    "    'FEED_URI': './scraper_results/NS538.json',\n",
    "    'LOG_ENABLED': False,\n",
    "    'ROBOTSTXT_OBEY': True,\n",
    "    'USER_AGENT': 'Thapani Sawaengsri (thapani.sawaengsri@gmail.com)',\n",
    "    'AUTOTHROTTLE_ENABLED': True,\n",
    "    'HTTPCACHE_ENABLED': True\n",
    "})\n",
    "\n",
    "# Instantiate and start scraper\n",
    "process.crawl(FiveThirtyEight)\n",
    "process.start()\n",
    "print('Success')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(26, 4)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>title</th>\n",
       "      <th>article_link</th>\n",
       "      <th>author</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaT</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaT</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2019-10-25</td>\n",
       "      <td>\\n\\t\\t\\t\\tWhere The Public Stands On Impeachme...</td>\n",
       "      <td>https://fivethirtyeight.com/features/where-the...</td>\n",
       "      <td>Nathaniel Rakich</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2019-10-24</td>\n",
       "      <td>\\n\\t\\t\\t\\tNine Candidates Have Made The Novemb...</td>\n",
       "      <td>https://fivethirtyeight.com/features/klobuchar...</td>\n",
       "      <td>Nathaniel Rakich</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2019-10-23</td>\n",
       "      <td>\\n\\t\\t\\t\\tWe’ve Already Seen Twice As Many Pre...</td>\n",
       "      <td>https://fivethirtyeight.com/features/weve-alre...</td>\n",
       "      <td>Nathaniel Rakich</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        date                                              title  \\\n",
       "0        NaT                                               None   \n",
       "1        NaT                                               None   \n",
       "2 2019-10-25  \\n\\t\\t\\t\\tWhere The Public Stands On Impeachme...   \n",
       "3 2019-10-24  \\n\\t\\t\\t\\tNine Candidates Have Made The Novemb...   \n",
       "4 2019-10-23  \\n\\t\\t\\t\\tWe’ve Already Seen Twice As Many Pre...   \n",
       "\n",
       "                                        article_link            author  \n",
       "0                                               None              None  \n",
       "1                                               None              None  \n",
       "2  https://fivethirtyeight.com/features/where-the...  Nathaniel Rakich  \n",
       "3  https://fivethirtyeight.com/features/klobuchar...  Nathaniel Rakich  \n",
       "4  https://fivethirtyeight.com/features/weve-alre...  Nathaniel Rakich  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_json('./scraper_results/NS538.json', orient='records')\n",
    "\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like the scraper worked! The first two blank rows are article-like areas that were not actually articles. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
