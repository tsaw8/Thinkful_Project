{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment\n",
    "\n",
    "In this assignment, we'll continue working with the house prices data. We will complete the following tasks:\n",
    "\n",
    "* Load the **houseprices** data from Thinkful's database.\n",
    "* Reimplement your model from the previous checkpoint.\n",
    "* Try OLS, Lasso, Ridge, and ElasticNet regression using the same model specification. This time, you need to do **k-fold cross-validation** to choose the best hyperparameter values for your models. Which model is the best? Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## House price model \n",
    "\n",
    "In this assignment, we will revisit the house price model and focus on the problem of __overfitting__ and __underfitting__. Overfitting refers to a model that models the training data too well and is unable to generalize to new data, thus producing poor results. On the other hand, an underfitted model will have poor performance on the training data and new data. A good model should be able to __generalize__ and reduce the __generalization gap__ which is the difference between the erros in the test and training set. As a general rule, if our model is too complex, it will tend to overfit. Inversely, if our model is not complex enough, it will underfit the training set.\n",
    "\n",
    "We will also try different regression model, which incorportate __regularization__, the process of modifying algorithms in order to lower the generalization gap without sacrificing training performance.\n",
    "\n",
    "When choosing the best hyperparameter values for our models, we will use __k-fold cross-validation__. We will split the data into 5 random folds to estimate the skill fo the model on unseen data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iteration 1 OLS\n",
    "\n",
    "We'll start by reloading our houseprice model and analyzing the performance of Ordinary Least Squares. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries \n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.tools.eval_measures import mse, rmse\n",
    "from sklearn.linear_model import LinearRegression, LassoCV, RidgeCV, ElasticNetCV\n",
    "from sqlalchemy import create_engine\n",
    "import warnings\n",
    "\n",
    "# Import data\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "postgres_user = 'dsbc_student'\n",
    "postgres_pw = '7*.8G9QH21'\n",
    "postgres_host = '142.93.121.174'\n",
    "postgres_port = '5432'\n",
    "postgres_db = 'houseprices'\n",
    "\n",
    "engine = create_engine('postgresql://{}:{}@{}:{}/{}'.format(\n",
    "    postgres_user, postgres_pw, postgres_host, postgres_port, postgres_db))\n",
    "\n",
    "houseprices_df = pd.read_sql_query('select * from houseprices',con=engine)\n",
    "\n",
    "# no need for an open connection, \n",
    "# as we're only doing a single query\n",
    "engine.dispose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dummy variables \n",
    "houseprices_df = pd.concat([houseprices_df,pd.get_dummies(houseprices_df.mszoning, prefix=\"mszoning\", drop_first=True)], axis=1)\n",
    "houseprices_df = pd.concat([houseprices_df,pd.get_dummies(houseprices_df.housestyle, prefix=\"style\", drop_first=True)], axis=1)\n",
    "\n",
    "dummy_column_names = list(pd.get_dummies(houseprices_df.mszoning, prefix=\"mszoning\", drop_first=True).columns)\n",
    "dummy_column_names = dummy_column_names + list(pd.get_dummies(houseprices_df.housestyle, prefix=\"style\", drop_first=True).columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new variables \n",
    "houseprices_df['totalsf'] = houseprices_df['totalbsmtsf'] + houseprices_df['firstflrsf'] + houseprices_df['secondflrsf']\n",
    "\n",
    "houseprices_df['int_over_sf'] = houseprices_df['totalsf'] * houseprices_df['overallqual']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Y is the target variable\n",
    "Y = np.log1p(houseprices_df['saleprice'])\n",
    "# X is the feature set\n",
    "X = houseprices_df[['overallqual','grlivarea','garagearea','totalbsmtsf','firstflrsf', 'int_over_sf'] + dummy_column_names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of observations in training set is 1168\n",
      "The number of observations in test set is 292\n"
     ]
    }
   ],
   "source": [
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size = 0.2, random_state = 465)\n",
    "\n",
    "print(\"The number of observations in training set is {}\".format(X_train.shape[0]))\n",
    "print(\"The number of observations in test set is {}\".format(X_test.shape[0]))\n",
    "\n",
    "alphas = [np.power(10.0,p) for p in np.arange(-10,40,1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R-squared of the model in the training set is: 0.8292006059392802\n",
      "-----Test set statistics-----\n",
      "R-squared of the model in the test set is: 0.820784417090392\n",
      "Mean absolute error of the prediction is: 0.12885874941196004\n",
      "Mean squared error of the prediction is: 0.02988341865036109\n",
      "Root mean squared error of the prediction is: 0.1728682117983555\n",
      "Mean absolute percentage error of the prediction is: 1.0765115436158368\n"
     ]
    }
   ],
   "source": [
    "# We fit an OLS model using sklearn\n",
    "lrm = LinearRegression()\n",
    "lrm.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "# We are making predictions here\n",
    "y_preds_train = lrm.predict(X_train)\n",
    "y_preds_test = lrm.predict(X_test)\n",
    "\n",
    "print(\"R-squared of the model in the training set is: {}\".format(lrm.score(X_train, y_train)))\n",
    "print(\"-----Test set statistics-----\")\n",
    "print(\"R-squared of the model in the test set is: {}\".format(lrm.score(X_test, y_test)))\n",
    "print(\"Mean absolute error of the prediction is: {}\".format(mean_absolute_error(y_test, y_preds_test)))\n",
    "print(\"Mean squared error of the prediction is: {}\".format(mse(y_test, y_preds_test)))\n",
    "print(\"Root mean squared error of the prediction is: {}\".format(rmse(y_test, y_preds_test)))\n",
    "print(\"Mean absolute percentage error of the prediction is: {}\".format(np.mean(np.abs((y_test - y_preds_test) / y_test)) * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we see, the R-squared of the model in the training set is 0.83 and is 0.82 in the test set. The difference between these values is very small so our model is a good fit in the training set. \n",
    "\n",
    "We also printed out some prediction statistics on the test set to compare with the following models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iteration 2 Lasso \n",
    "\n",
    "Least Absolute Shrinkage and Selection Operator regression works to prevent overfitting by trying to penaltize non-zero coefficients and the sum of their absolute values, forcing small parameter estimates to be equal to zero, effectively dropping them from the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R-squared of the model on the training set is: 0.8289842776415911\n",
      "-----Test set statistics-----\n",
      "R-squared of the model on the test set is: 0.8184967365411038\n",
      "Mean absolute error of the prediction is: 0.1292773499289438\n",
      "Mean squared error of the prediction is: 0.030264879427837916\n",
      "Root mean squared error of the prediction is: 0.17396804139794733\n",
      "Mean absolute percentage error of the prediction is: 1.0804753132610663\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "lassoregr = LassoCV(alphas=alphas, cv=5)\n",
    "lassoregr.fit(X_train, y_train)\n",
    "\n",
    "# We are making predictions here\n",
    "y_preds_train = lassoregr.predict(X_train)\n",
    "y_preds_test = lassoregr.predict(X_test)\n",
    "\n",
    "print(\"R-squared of the model on the training set is: {}\".format(lassoregr.score(X_train, y_train)))\n",
    "print(\"-----Test set statistics-----\")\n",
    "print(\"R-squared of the model on the test set is: {}\".format(lassoregr.score(X_test, y_test)))\n",
    "print(\"Mean absolute error of the prediction is: {}\".format(mean_absolute_error(y_test, y_preds_test)))\n",
    "print(\"Mean squared error of the prediction is: {}\".format(mse(y_test, y_preds_test)))\n",
    "print(\"Root mean squared error of the prediction is: {}\".format(rmse(y_test, y_preds_test)))\n",
    "print(\"Mean absolute percentage error of the prediction is: {}\".format(np.mean(np.abs((y_test - y_preds_test) / y_test)) * 100))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we see, the R-squared of the model in the training set is 0.83 and is 0.82 in the test set. The difference between these values is very similar to the OLS model. However, all of the evaluation metrics increased from the previous model indicating slightly more errors in this model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iteration 3 Ridge\n",
    "\n",
    "Ridge regression minimizes this cost function by imposing a pentality for large coefficients. As the complexity of a model increases and features correlate with one another more and more, the model is incorporating too much variance in the training set. Removing features from the model can be seen as settings their coefficients to zero. Instead of forcing them to be exactly zero, let's penalize them if they are too far from zero, thus enforcing them to be small in a continuous way. This way, we decrease model complexity while keeping all variables in the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R-squared of the model on the training set is: 0.8286748212734817\n",
      "-----Test set statistics-----\n",
      "R-squared of the model on the test set is: 0.8167057471604998\n",
      "Mean absolute error of the prediction is: 0.12963708370240168\n",
      "Mean squared error of the prediction is: 0.030563519114130914\n",
      "Root mean squared error of the prediction is: 0.17482425207656663\n",
      "Mean absolute percentage error of the prediction is: 1.0837971354032898\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "# Fitting a ridge regression model. Alpha is the regularization\n",
    "# parameter (usually called lambda). As alpha gets larger, parameter\n",
    "# shrinkage grows more pronounced.\n",
    "ridgeregr = RidgeCV(alphas=alphas, cv=5) \n",
    "ridgeregr.fit(X_train, y_train)\n",
    "\n",
    "# We are making predictions here\n",
    "y_preds_train = ridgeregr.predict(X_train)\n",
    "y_preds_test = ridgeregr.predict(X_test)\n",
    "\n",
    "print(\"R-squared of the model on the training set is: {}\".format(ridgeregr.score(X_train, y_train)))\n",
    "print(\"-----Test set statistics-----\")\n",
    "print(\"R-squared of the model on the test set is: {}\".format(ridgeregr.score(X_test, y_test)))\n",
    "print(\"Mean absolute error of the prediction is: {}\".format(mean_absolute_error(y_test, y_preds_test)))\n",
    "print(\"Mean squared error of the prediction is: {}\".format(mse(y_test, y_preds_test)))\n",
    "print(\"Root mean squared error of the prediction is: {}\".format(rmse(y_test, y_preds_test)))\n",
    "print(\"Mean absolute percentage error of the prediction is: {}\".format(np.mean(np.abs((y_test - y_preds_test) / y_test)) * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These results are very similar to the Lasso model. The R-squared changes from 0.83 in the training model to 0.82 in the testing set. The evaluation metrics are still higher than the OLS model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iteration 4 Elastic Net \n",
    "\n",
    "Elastic net is a regularized regression method that linearly combines the L1 and L2 penalties of the lasso and ridge methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R-squared of the model on the training set is: 0.8291190450186339\n",
      "-----Test set statistics-----\n",
      "R-squared of the model on the test set is: 0.8193730067494465\n",
      "Mean absolute error of the prediction is: 0.1291282364967868\n",
      "Mean squared error of the prediction is: 0.03011876518340893\n",
      "Root mean squared error of the prediction is: 0.17354758766231507\n",
      "Mean absolute percentage error of the prediction is: 1.0790605176978927\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import ElasticNet\n",
    "\n",
    "elasticregr = ElasticNetCV(alphas=alphas, cv=5)\n",
    "\n",
    "elasticregr.fit(X_train, y_train)\n",
    "# We are making predictions here\n",
    "y_preds_train = elasticregr.predict(X_train)\n",
    "y_preds_test = elasticregr.predict(X_test)\n",
    "\n",
    "print(\"R-squared of the model on the training set is: {}\".format(elasticregr.score(X_train, y_train)))\n",
    "print(\"-----Test set statistics-----\")\n",
    "print(\"R-squared of the model on the test set is: {}\".format(elasticregr.score(X_test, y_test)))\n",
    "print(\"Mean absolute error of the prediction is: {}\".format(mean_absolute_error(y_test, y_preds_test)))\n",
    "print(\"Mean squared error of the prediction is: {}\".format(mse(y_test, y_preds_test)))\n",
    "print(\"Root mean squared error of the prediction is: {}\".format(rmse(y_test, y_preds_test)))\n",
    "print(\"Mean absolute percentage error of the prediction is: {}\".format(np.mean(np.abs((y_test - y_preds_test) / y_test)) * 100))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The elastic net model also yield very similar results to the ridge and lasso regression models. The R-squared and evaulation metrics of this model is somewhere between the ridge and lasso regression since it is a combination of both models. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Which model is the best?\n",
    "I would continue with the OLS regression since its R-squared is higher than the other models meaning that the OLS is able to explain more variation in the target variable. The evaluation metrics of the OLS is slightly lower than the others, signifying a better fit line. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concept Summary \n",
    "\n",
    "Linear models that contain many features or variables that are correlated to one another, the standard OLS parameters estimates will have high variance, thus making the model unreliable. To counter this, we can use regularization, which will allow us to decrease the variance at at cost of introducing some bias. \n",
    "\n",
    "Three popular regularization techniques that aim at decreasing the size of the coefficients include:\n",
    " * Ride Regression: penalizes sum of squared coefficents (L2 pentalty)\n",
    " * Lasso Regression: penalizes the sum of absolute values of the coefficients (L1 penalty)\n",
    " * Elastic Net: combination of Ridge and Lasso"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
