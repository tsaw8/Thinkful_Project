{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tweeter Bot Detection\n",
    "\n",
    "## Integrating machine learning to detect bots\n",
    "\n",
    "Over the past ten plus years, Twitter has explosively evolved into a major communication hub. \"Its primary purpose is to connect people and allow people to share their thoughts with a big audience. Twitter can also be a very helpful platform for growing a following and providing your audience with valuable content before they even become customers\" [Hubspot](https://blog.hubspot.com/marketing/what-is-twitter). However, not all accounts are geninue users. According to a Twitter SEC filling in 2017, Twitter estimated 8.5% of all users to be bots. To validate the credibility of communication exchanged on the platform, efforts in idnetifying spam bots will help improve user's experience on twitter. \n",
    "\n",
    "In this project, we will be using [Cresci-2017](https://botometer.iuni.iu.edu/bot-repository/datasets.html) bot repository datasets to detect bot accounts. We'll begin with exploring traits between geniuine and bot accounts. Then, we will imploy supervised learning models (Logistic Regression, Random Forest, Stoachtic Gradient Boosting) to create a Twitter classifer. Finally, we'll use clustering to identify traits among geniue and spam bot accounts. \n",
    "\n",
    " \n",
    "### Overview of Data\n",
    "There are a total of 5 files: \n",
    " * 1 example submission files \n",
    " * 2 transaction files (test and train)\n",
    " * 2 identity files (test and train) \n",
    " \n",
    " We will be merging train transaction and train identity to gain more information regarding detecting fraud. To keep things simple, we will only be using the training sets. Below is a [description](https://www.kaggle.com/c/ieee-fraud-detection/discussion/101203) of the attributes in each table. \n",
    " \n",
    "__Transaction Table__\n",
    "\n",
    "- TransactionDT: timedelta from a given reference datetime (not an actual timestamp)\n",
    "- TransactionAMT: transaction payment amount in USD\n",
    "- ProductCD: product code, the product for each transaction\n",
    "- card1 - card6: payment card information, such as card type, card category, issue bank, country, etc.\n",
    "- addr: address\n",
    "- dist: distance\n",
    "- P_ and (R__) emaildomain: purchaser and recipient email domain\n",
    "- C1-C14: counting, such as how many addresses are found to be associated with the payment card, etc. The actual meaning is masked.\n",
    "- D1-D15: timedelta, such as days between previous transaction, etc.\n",
    "- M1-M9: match, such as names on card and address, etc.\n",
    "- Vxxx: Vesta engineered rich features, including ranking, counting, and other entity relations.\n",
    "- Categorical Features:\n",
    "- ProductCD\n",
    "- card1 - card6\n",
    "- addr1, addr2\n",
    "- Pemaildomain Remaildomain\n",
    "- M1 - M9\n",
    "\n",
    "__Identity Table__\n",
    "\n",
    "Variables in this table are identity information – network connection information (IP, ISP, Proxy, etc) and digital signature (UA/browser/os/version, etc) associated with transactions.\n",
    "They're collected by Vesta’s fraud protection system and digital security partners.\n",
    "(The field names are masked and pairwise dictionary will not be provided for privacy protection and contract agreement)\n",
    "\n",
    "Categorical Features:\n",
    "- DeviceType\n",
    "- DeviceInfo\n",
    "- id12 - id38\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numpy and pandas\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "#Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import missingno as msno\n",
    "\n",
    "# Statistics tools\n",
    "import scipy.stats as stats\n",
    "\n",
    "# Sklearn data clean\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Model selection\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# Logistic Regression\n",
    "from sklearn.linear_model import Lasso, LogisticRegression\n",
    "\n",
    "# KNN Classifer \n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Decision Trees\n",
    "from sklearn import tree\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from IPython.display import Image\n",
    "import pydotplus\n",
    "import graphviz\n",
    "\n",
    "# Random Forests \n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# SVM\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Gradient Boost\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# Evaluate\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import log_loss,accuracy_score, f1_score,roc_auc_score, confusion_matrix, classification_report\n",
    "\n",
    "# Hyperparameter tuning\n",
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n",
    "\n",
    "# Datetime\n",
    "from datetime import datetime\n",
    "\n",
    "# Import data\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import genuine accounts \n",
    "g_users = pd.read_csv('/Users/tsawaengsri/Desktop/Data Science Courses/Datasets/cresci-2017.csv/datasets_full.csv/genuine_accounts.csv/users.csv')\n",
    "\n",
    "# Import spam bot accounts\n",
    "soc_bot1_users = pd.read_csv('/Users/tsawaengsri/Desktop/Data Science Courses/Datasets/cresci-2017.csv/datasets_full.csv/social_spambots_1.csv/users.csv')\n",
    "soc_bot2_users = pd.read_csv('/Users/tsawaengsri/Desktop/Data Science Courses/Datasets/cresci-2017.csv/datasets_full.csv/social_spambots_2.csv/users.csv')\n",
    "soc_bot3_users = pd.read_csv('/Users/tsawaengsri/Desktop/Data Science Courses/Datasets/cresci-2017.csv/datasets_full.csv/social_spambots_3.csv/users.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Genuine tweets\n",
      "(3474, 42)\n",
      "---------------\n",
      "Spam bot tweets\n",
      "(991, 41)\n",
      "(3457, 40)\n",
      "(464, 41)\n"
     ]
    }
   ],
   "source": [
    "print('Genuine tweets')\n",
    "print(g_users.shape)\n",
    "\n",
    "print('---------------')\n",
    "\n",
    "print('Spam bot tweets')\n",
    "print(soc_bot1_users.shape)\n",
    "print(soc_bot2_users.shape)\n",
    "print(soc_bot3_users.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3457 entries, 0 to 3456\n",
      "Data columns (total 40 columns):\n",
      "id                                    3457 non-null int64\n",
      "name                                  3457 non-null object\n",
      "screen_name                           3457 non-null object\n",
      "statuses_count                        3457 non-null int64\n",
      "followers_count                       3457 non-null int64\n",
      "friends_count                         3457 non-null int64\n",
      "favourites_count                      3457 non-null int64\n",
      "listed_count                          3457 non-null int64\n",
      "url                                   10 non-null object\n",
      "lang                                  3457 non-null object\n",
      "time_zone                             14 non-null object\n",
      "location                              13 non-null object\n",
      "default_profile                       15 non-null float64\n",
      "default_profile_image                 46 non-null float64\n",
      "geo_enabled                           5 non-null float64\n",
      "profile_image_url                     3457 non-null object\n",
      "profile_banner_url                    15 non-null object\n",
      "profile_use_background_image          3457 non-null int64\n",
      "profile_background_image_url_https    3457 non-null object\n",
      "profile_text_color                    3457 non-null object\n",
      "profile_image_url_https               3457 non-null object\n",
      "profile_sidebar_border_color          3457 non-null object\n",
      "profile_background_tile               3437 non-null float64\n",
      "profile_sidebar_fill_color            3457 non-null object\n",
      "profile_background_image_url          3457 non-null object\n",
      "profile_background_color              3457 non-null object\n",
      "profile_link_color                    3457 non-null object\n",
      "utc_offset                            14 non-null float64\n",
      "is_translator                         0 non-null float64\n",
      "follow_request_sent                   0 non-null float64\n",
      "protected                             0 non-null float64\n",
      "verified                              0 non-null float64\n",
      "notifications                         0 non-null float64\n",
      "description                           14 non-null object\n",
      "contributors_enabled                  0 non-null float64\n",
      "following                             0 non-null float64\n",
      "created_at                            3457 non-null object\n",
      "timestamp                             3457 non-null object\n",
      "crawled_at                            3457 non-null object\n",
      "updated                               3457 non-null object\n",
      "dtypes: float64(12), int64(7), object(21)\n",
      "memory usage: 1.1+ MB\n"
     ]
    }
   ],
   "source": [
    "soc_bot2_users.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select common columns \n",
    "g_users = g_users.loc[:,[\"id\", \"name\", \"screen_name\", \"statuses_count\", \"followers_count\", \"friends_count\", \"lang\", \"default_profile\", \"protected\", \"verified\", \"description\", \"contributors_enabled\"]]\n",
    "soc_bot1_users = soc_bot1_users.loc[:,[\"id\", \"name\", \"screen_name\", \"statuses_count\", \"followers_count\", \"friends_count\", \"lang\", \"default_profile\", \"protected\", \"verified\", \"description\", \"contributors_enabled\"]]\n",
    "soc_bot2_users = soc_bot2_users.loc[:,[\"id\", \"name\", \"screen_name\", \"statuses_count\", \"followers_count\", \"friends_count\", \"lang\", \"default_profile\", \"protected\", \"verified\", \"description\", \"contributors_enabled\"]]\n",
    "soc_bot3_users = soc_bot3_users.loc[:,[\"id\", \"name\", \"screen_name\", \"statuses_count\", \"followers_count\", \"friends_count\", \"lang\", \"default_profile\", \"protected\", \"verified\", \"description\", \"contributors_enabled\"]]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merging datasets\n",
    "The transaction training file lacks identity information on each transaction, so we will merge the training identity and transaction files on TransactionID. Since each observation has a unique transaction ID, we will do a 1 to 1 join. We will be performing a left join since our focus remains on the transaction table.\n",
    "\n",
    "Below, we can see that the row length for train transaction and identity are not equal. It was noted that Vesta was unable to obtain all identity information so we will continue with the merge table for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b_users = pd.concat([soc_bot1_users,soc_bot2_users,soc_bot3_users], ignore_index=True, sort=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create tweet class, 1 for bot and 0 for genuine tweets\n",
    "b_users['class'] = 1\n",
    "g_users['class'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate df\n",
    "df = pd.concat([b_users,g_users], ignore_index=True, sort=False)\n",
    "\n",
    "# Randomly shuffle df \n",
    "df = df.reindex(np.random.permutation(df.index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Missing Values\n",
    "Now, we will take a look at missing values in each column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate missing values by column\n",
    "def missing_values_table(df):\n",
    "        # Total missing values\n",
    "        mis_val = df.isnull().sum()\n",
    "        \n",
    "        # Percentage of missing values\n",
    "        mis_val_percent = 100 * df.isnull().sum() / len(df)\n",
    "        \n",
    "        # Make a table with the results\n",
    "        mis_val_table = pd.concat([mis_val, mis_val_percent], axis=1)\n",
    "        \n",
    "        # Rename the columns\n",
    "        mis_val_table_ren_columns = mis_val_table.rename(\n",
    "        columns = {0 : 'Missing Values', 1 : '% of Total Values'})\n",
    "        \n",
    "        # Sort the table by percentage of missing descending\n",
    "        mis_val_table_ren_columns = mis_val_table_ren_columns[\n",
    "            mis_val_table_ren_columns.iloc[:,1] != 0].sort_values(\n",
    "        '% of Total Values', ascending=False).round(1)\n",
    "        \n",
    "        # Print some summary information\n",
    "        print (\"Your selected dataframe has \" + str(df.shape[1]) + \" columns.\\n\"      \n",
    "            \"There are \" + str(mis_val_table_ren_columns.shape[0]) +\n",
    "              \" columns that have missing values.\")\n",
    "        \n",
    "        # Return the dataframe with missing information\n",
    "        return mis_val_table_ren_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_values_table(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "msno.matrix(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As shown in the matrix, many columns such as truncated, geo, contributors, etc. appear to be missing all datapoints. The data doesn't appear to be missing at random since there is a repetitive pattern between those columns. \n",
    "\n",
    "We will drop columns missing more than 90% of data points and in_reply_to_screen_name since that isn't our focus. Then, we will drop the remaining rows with missing values since that is only a small percentage of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get columns with >= 80% missing\n",
    "missing_df = missing_values_table(df)\n",
    "missing_columns = list(missing_df[missing_df['% of Total Values'] >= 90].index)\n",
    "print('We will drop %d columns.' % len(missing_columns))\n",
    "print('Drop columns: ', missing_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(labels=['truncated', 'geo', 'contributors', 'favorited', 'retweeted', 'possibly_sensitive', 'place','in_reply_to_screen_name'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop all rows that have any NaN values\n",
    "df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_values_table(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Time Series \n",
    "\n",
    "Time series data is used when we want to analyze or explore variation over time. This is useful when exploring Twitter text data if we want to track the prevalence of a word or set of words.\n",
    "\n",
    "Let's convert timestamp into time datatype. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print created_at to see the original format of datetime in Twitter data\n",
    "print(df['created_at'].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the created_at column to np.datetime object\n",
    "df['created_at'] = pd.to_datetime(df['created_at'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print created_at to see new format\n",
    "print(df['created_at'].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the index of df to created_at\n",
    "df = df.set_index('created_at')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis\n",
    "Exploratory Data Analysis (EDA) is an iterative process to explore the data and summarize characteristics by calculating statistics or visualize methods. The purpose of EDA is gain an understanding of the data by identifying trends, anomalies, or relationships that might be helpful when making decisions in the modeling process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bot vs human tweets \n",
    "counts = df['class'].value_counts()\n",
    "human = counts[0]\n",
    "bot = counts[1]\n",
    "human_per = (human/(human + bot))*100\n",
    "bot_per = (bot/(human + bot))*100\n",
    "print('There are {} tweets made by humans({:.3f}%) and {} tweets made by bots ({:.3f}%) in this dataset.'.format(human, human_per, bot, bot_per))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot target variable\n",
    "plt.figure(figsize=(12,6))\n",
    "g = sns.countplot(x = 'class', data = df)\n",
    "g.set_title('Count of Tweets made by Humans vs Bots', fontsize = 17)\n",
    "g.set_xlabel('User Type', fontsize = 15)\n",
    "g.set_ylabel('Tweets', fontsize = 15)\n",
    "\n",
    "for p in g.patches:\n",
    "    height = p.get_height()\n",
    "    g.text(p.get_x()+p.get_width()/2.,\n",
    "            height + 3,\n",
    "            '{:1.2f}%'.format(height/len(df) * 100),\n",
    "            ha=\"center\", fontsize=15) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "account = df.groupby(by='user_id', as_index=False).agg({'class':pd.Series.nunique})\n",
    "account.describe()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Total user accounts \n",
    "account = df.groupby(by='user_id', as_index=False).agg({'class':pd.Series.nunique})\n",
    "human = account[0]\n",
    "bot = account[1]\n",
    "human_per = (human/(human + bot))*100\n",
    "bot_per = (bot/(human + bot))*100\n",
    "print('There are {} total accounts: {} are human({:.3f}%) and {} are bots({:.3f}%).'.format(len(account),human, human_per,bot,bot_per))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a python column\n",
    "ds_tweets['python'] = check_word_in_tweet('#python', ds_tweets)\n",
    "\n",
    "# Create an rstats column\n",
    "ds_tweets['rstats'] = check_word_in_tweet('#rstats', ds_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average of bot tweet per day\n",
    "bt_per_day = df['class'].resample('1 d').mean()\n",
    "\n",
    "# Average of genuine tweet per day\n",
    "#gt_per_day = df['class'==0].resample('1 d').mean()\n",
    "\n",
    "# Plot average tweet per day\n",
    "plt.plot( bt_per_day.index.day,bt_per_day, color = 'green')\n",
    "#plt.plot( gt_per_day.index.day,gt_per_day, color = 'blue')\n",
    "\n",
    "# Add labels and show\n",
    "plt.xlabel('Day'); plt.ylabel('Frequency')\n",
    "plt.title('Number of Tweets')\n",
    "#plt.legend(('Bot', 'Human'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tweets per day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tweet over time "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pauses between tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
